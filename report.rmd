---
classoption: a4paper, xcolor = usenames,dvipsnames
geometry: margin=2.5cm
bibliography: ref.bib
output:
  bookdown::pdf_document2:
    papersize: a4
    fig_caption: true
    highlight: tango
    keep_tex: true
    number_sections: true
    pandoc_args:
      - "--listings"
    toc_depth: 3
    toc: false
    latex_engine: lualatex
    includes:
      in_header: preamble.tex
      before_body: cover.tex
---

# Introduction

## Institut de Recherche en Informatique de Toulouse (IRIT)

### About the institut

The Institut de Recherche en Informatique de Toulouse (IRIT), created in 1990, is a Joint Research Unit (UMR 5505) of the Centre National de la Recherche Scientifique (CNRS), the Institut National Polytechnique de Toulouse (INP), the Université Paul Sabatier Toulouse3 (UT3), the Université Toulouse1 Capitole (UT1) and the Université de Toulouse Jean Jaurès (UT2J).

IRIT is one of the largest UMR at the national level, is one of the pillars of research in Occitanie with its 600 members, permanent and non-permanent, and about 100 external collaborators. Due to its multi-tutorial nature (CNRS, Toulouse Universities), its scientific impact and its interactions with other fields, the laboratory constitutes one of the structuring forces of the IT landscape and its applications in the digital world, both at regional and national level.

Through its cutting-edge work and dynamics, our unit has been able to define its identity and acquire undeniable visibility, while positioning itself at the heart of changes in local structures: University of Toulouse, as well as the various mechanisms resulting from future investments (LabEx CIMI, IRT Saint-Exupéry, SAT TTT, 3IA ANITI).

IRIT has focused its research on five major scientific issues and six strategic application areas.

- Health, Autonomy, Living, Well-being
- Smart City
- Aerospace and Transportation
- Social Media, Digital Social Ecosystems
- e-Education for learning and teaching
- Heritage and People Safety

As well as strategic action:

- Scientific Computing, Big Data and AI

### Organization

The 24 research groups of the laboratory are dispatched in seven scientific departments:

- Dpt ASR : Architecture, Systems, Networks
- Dpt CISO : HPC, Simulation, Optimization
- Dpt FSL : Reliability of Systems and Software
- Dpt GD : Data Management
- Dpt ICI : Interaction, Collective Intelligence
- Dpt IA : Artificial Intelligence
- Dpt SI : Signals, Images

## The internship

Markov decisions processes (MDPs) and their model free counterpart in reinforcement learning (RL) have known a large success in the last two decades. Although research in these two areas has been taking place for more than fifty years, the field gained momentum only recently following the advent of powerful hardware and algorithms with which supra­human performance were obtained in games like Chess or Go. However, these impressive successes often rely on quite exceptional hardware possibilities and cannot be applied in many ”usual” contexts, where, for instance, the volume of data available or the amount of computing power is more restricted. To define the next generation of more ”democratic” and widely applicable algorithms, such methods still need to deal with very demanding exploration issues as soon as the state/action spaces are not small. One way around this is to use underlying knowledge and structure present in many MDPs. This is especially true for problems related to scheduling and resources sharing in among others server farms, clouds, and cellular wireless networks. The internships will revolve around this theme of improving the efficiency of learning algorithms by leveraging the structure of the underlying problem and focus mainly on model­free approach.

# System settings

Two type of systems are studied in this internship: queuing system and load-balancing system.

## Queuing system

### Parameters

We have $n$ classes of queue $C_{1}, \dots, C_{n}$. For each class of queue, its behavior is fully controlled by which environment it is in. An environment can be in one of the states $m_{1}, \dots, m_{m}$. And the environment of class $C_{i}$ is a random variable, denoted by $M_{i}$, which is in one of the states $m_{1}, \dots, m_{m}$. For each environment, $C_{i}$ has their own holding cost $c_{i, j}$ (the cost of one unfinished unit of work on the queue), arrival rate $\lambda_{i, j}$ (the rate of one more unit of work arriving to the queue) and departure rate $\mu_{i, j}$ (the rate of one unit of work on the queue has finished). Furthermore, the environment $M_{i}$ can change from $m_{j}$ and $m_{k}$ with the rate of $\xi_{i, j, k}$. In this internship, we focus on the case where $n \coloneq 2$. A table summarizing all parameters is shown below.

```{=latex}
\begin{table}[ht]
\caption{Queuing system parameters}
\begin{center}
\begin{tabular}{c c c c c}
    \hline
    \multicolumn{2}{c}{} & $m_{1}$ & $\dots$ & $m_{m}$ \\
    \cline{2-5}
    \multirow{3}{*}{$C_{1}$} &  Holding cost & $c_{1, 1}$ & $\dots$ & $c_{1, m}$ \\
    & Arrival rate & $\lambda_{1, 1}$ & $\dots$ & $\lambda_{1, m}$ \\
    & Departure rate & $\mu_{1, 1}$ & $\dots$ & $\mu_{1, m}$ \\
    \cline{2-5}
    $\vdots$  \\
    \cline{2-5}
    \multirow{3}{*}{$C_{n}$} &  Holding cost & $c_{n, 1}$ & $\dots$ & $c_{n, m}$ \\
    & Arrival rate & $\lambda_{n, 1}$ & $\dots$ & $\lambda_{n, m}$ \\
    & Departure rate & $\mu_{n, 1}$ & $\dots$ & $\mu_{n, m}$ \\
    \hline
\end{tabular}
\end{center}
\label{tab:qs-param}
\end{table}
```

For each $C_{i}$, we have a matrix transition as below.

```{=latex}
\begin{table}[ht]
\caption{Matrix transition of the environment of $C_{i}$}
\begin{center}
\begin{tabular}{c c c c}
    \hline
    & $m_{1}$ & $\dots$ & $m_{m}$ \\
    $m_{1}$ & $\xi_{i, 1, 1}$ & \dots & $\xi_{i, 1, m}$ \\
    \vdots \\
    $m_{m}$ & $\xi_{i, m, 1}$ & \dots & $\xi_{i, m, m}$ \\
    \hline
\end{tabular}
\end{center}
\label{tab:mat-transition-ci}
\end{table}
```

The state of the system is represented by two vectors:

- $S = (X_{1}, \dots, X_{n})$ where $X_{i}$ is a random variable represents the current number of works of class $C_{i}$ and is observable.
- $E = (M_{1}, \dots, M_{n})$ and this vector is not observable.

### Cost

The cost of the system is a function of $S$ and $E$. We propose two functions of cost. The first one is a simple linear function.

$$
f_{1}(S, E) = \sum_{1}^{n} c_{i, M_{i}} X_{i}
$$

And the second one is a convex function which is specialized for the case $n = 2$, where $\epsilon$ is a fixed positive constant.

$$
f_{2}(S, E) = c_{1, M_{1}} X_{1} + c_{2, M_{2}} (\epsilon X_{2}^{2} + X_{2})
$$

### Agent

The agent will decide which queues should be activated. His goal is to minimize the cost of the whole system. Only works on activating queues can be processed and finished. In more generic problems, the agent is allowed to activate / deactivate multiple queues at the same time based on some conditions. However, in this internship, the agent can only activate one queue at a time.

```{=latex}
\begin{figure}
\centering
\begin{tikzpicture}[>=latex]
% the shapes
\path
  (0,3cm) pic {queuei=1}
  (0,1cm) pic {queuei=2}
  (0,-2cm) pic {queuei=n};
\path
  (1,4cm) coordinate (aux1)
  (1,-3.5cm) coordinate (aux2)
  (-0.5,0cm) coordinate (aux3)
  (2.5,0cm) coordinate (aux4);
\node[draw,dashed,text width=2.5cm,fit={(aux1) (aux2) (aux3) (aux4)}] (dashed) {};
\node[draw,align=center,circle,inner sep=2pt]
  at (4,-1) (agent)
  {Agent};

%the arrows
\draw[->, line width=1.25]
  ([xshift=-3.5cm]q-1.west) --
    node[anchor=south,align=center] {$\lambda_{1, M_{1}}$}
  (q-1.west);
\draw[->]
  ([xshift=1.5cm]q-1.east) --
    node[anchor=south,align=center] {$\mu_{1, M_{1}}$}
  ([xshift=4.5cm]q-1.east);
\draw[->, line width=1.25]
  ([xshift=-3.5cm]q-2.west) --
    node[anchor=south,align=center] {$\lambda_{2, M_{2}}$}
  (q-2.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-2.east) --
    node[anchor=south,align=center] {$\mu_{2, M_{2}}$}
  ([xshift=4.5cm]q-2.east);
\draw[->, line width=1.25]
  ([xshift=-3.5cm]q-n.west) --
    node[anchor=south,align=center] {$\lambda_{n, M_{n}}$}
  (q-n.west);
\draw[->]
  ([xshift=1.5cm]q-n.east) --
    node[anchor=south,align=center] {$\mu_{n, M_{n}}$}
  ([xshift=4.5cm]q-n.east);
\draw[->,dotted]
  (agent.west) --
    node[anchor=south,align=center] {}
  ([xshift=1.5cm]q-1.east);
\draw[->,densely dotted, line width=1]
  (agent.west) --
    node[anchor=south,align=center] {}
  ([xshift=1.5cm]q-2.east);
\draw[->,dotted]
  (agent.west) --
    node[anchor=south,align=center] {}
  ([xshift=1.5cm]q-n.east);
\path ([xshift=0.5cm]q-2.south) -- ([xshift=0.5cm]q-n.north) node [font=\LARGE, midway, sloped] {$\dots$};
\end{tikzpicture}
\caption[Visualization of the queueing system when the agent activates queue 2]{Visualization of the queueing system when the agent activates queue 2, the bold lines represent the flow of works inside the system.}
\end{figure}
```

### Evolution

In the continuos-time scale, each type of event (work arrival, work departure and environment changing) happens independently. That system is quite hard to program, therefore, we used a technique called "uniformization" to move from the continuos-time scale to discrete-time scale. In this time scale, at a given time, only one event can take place, regardless of their type.

In particular, given the system state is $(S, E)$, the agent decides to activate queue $a$. One of these $n + 1 + n(m - 1) + 1$ events can happen.

- The work of class $C_{i}$ increases by 1. Because there are $n$ classes, we have $n$ events of this kind with the rate $\lambda_{1, M_{1}}, \dots, \lambda_{n, M_{n}}$ respectively (if the number of work of class $C_{i}$ reaches an upper limit, we consider $\lambda_{i, M_{i}} = 0$).
- The work of class $C_{a}$ decreases by 1. Because we can only activate only one class, there is only one event of this type and its rate is $\mu_{a, M_{a}}$.
- The environment of class $C_{i}$ changes to a different environment other than $m_{i}$ and the rate changing to environment $j$ is $\xi_{i, M_{i}, j}$. Because there are $m - 1$ possible changes for each class and there are $n$ classes, the number of this kind of event is $n(m - 1)$.
- And a special dummy event where nothing changes.

A discrete probability distribution is used to express that. In order to satisfy the condition of a probability distribution, all the rates above are divided by a normalization constant $C$ to make sure that their sum are not greater than 1. If that sum is smaller than 1, the special dummy event is used to fill the gap so that the final sum will be equal to 1.

The normalization constant has the form as follow, which is deduced from the above evolution of the system.

$$
C = \sum_{i = 1}^{n} \max_{j} \lambda_{i, j} + \max_{i, j} \mu_{i, j} + \sum_{i = 1}^{n} \max_{j} \sum_{k = 1, k \neq j}^{m} \xi_{i, j, k} + \epsilon
$$

If $\epsilon > 0$, the probability of the dummy event will always be greater than 0. In this internship, as we do not want the system evolves too slowly, we choose $\epsilon \coloneq 0$.

After obtaining all the information above, we use that discrete probability distribution to obtain the next transition $T$ of the system and denote $S'$ and $E'$ the next state of the system.

## Load-balancing system

### Parameters

For this system, we have a similar parameters with the one of the queuing system. The only difference in this system is that instead of having multiple arrival rate that depends on the class of queue as well as the environment, we have only one global arrival rate $\lambda$. A table summarizing the parameters of this system is shown below.

```{=latex}
\begin{table}[ht]
\caption{Load-balancing system parameters}
\begin{center}
\begin{tabular}{c c c c c}
    \hline
    \multicolumn{2}{c}{} & $m_{1}$ & $\dots$ & $m_{m}$ \\
    \cline{2-5}
    \multirow{2}{*}{$C_{1}$} &  Holding cost & $c_{1, 1}$ & $\dots$ & $c_{1, m}$ \\
    & Departure rate & $\mu_{1, 1}$ & $\dots$ & $\mu_{1, m}$ \\
    \cline{2-5}
    $\vdots$  \\
    \cline{2-5}
    \multirow{2}{*}{$C_{n}$} &  Holding cost & $c_{n, 1}$ & $\dots$ & $c_{n, m}$ \\
    & Departure rate & $\mu_{n, 1}$ & $\dots$ & $\mu_{n, m}$ \\
    \cline{2-5}
    & Global arrival rate & & $\lambda$ & \\
    \hline
\end{tabular}
\end{center}
\label{tab:lbs-param}
\end{table}
```

### Cost

Same as above, the cost of the system is a function of $S$ and $E$. However, in this system, only the simple linear version is used.

$$
f_{1}(S, E) = \sum_{1}^{n} c_{i, M_{i}} X_{i}
$$

### Agent

The goal of the agent is the same: minimizing the cost of the whole system. The difference here is instead of having to deactivating some queues, all queues now run continuously. His mission is to choose which queue to send the new arriving work to. Only the chosen queue will have new work arriving, the other queues only have to process the remaining works of them.

```{=latex}
\begin{figure}
\centering
\begin{tikzpicture}[>=latex]
% the shapes
\path
  (0,3cm) pic {queuei=1}
  (0,1cm) pic {queuei=2}
  (0,-2cm) pic {queuei=n};
\path
  (1,4cm) coordinate (aux1)
  (1,-3.5cm) coordinate (aux2)
  (-0.5,0cm) coordinate (aux3)
  (2.5,0cm) coordinate (aux4);
\node[draw,dashed,text width=2.5cm,fit={(aux1) (aux2) (aux3) (aux4)}] (dashed) {};
\node[draw,align=center,circle,inner sep=2pt]
  at (-2,0) (agent)
  {Agent};

%the arrows
\draw[->, line width=1.25]
  ([xshift=-1cm]agent.west) --
    node[anchor=south,align=center] {$\lambda$}
  (agent.west);
\draw[->, dotted]
  (agent.east) --
    node[anchor=south,align=center] {}
  (q-1.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-1.east) --
    node[anchor=south,align=center] {$\mu_{1, M_{1}}$}
  ([xshift=4.5cm]q-1.east);
\draw[->, densely dotted, line width=1]
  (agent.east) --
    node[anchor=south,align=center] {}
  (q-2.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-2.east) --
    node[anchor=south,align=center] {$\mu_{2, M_{2}}$}
  ([xshift=4.5cm]q-2.east);
\draw[->, dotted]
  (agent.east) --
    node[anchor=south,align=center] {}
  (q-n.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-n.east) --
    node[anchor=south,align=center] {$\mu_{n, M_{n}}$}
  ([xshift=4.5cm]q-n.east);
\path ([xshift=0.5cm]q-2.south) -- ([xshift=0.5cm]q-n.north) node [font=\LARGE, midway, sloped] {$\dots$};
\end{tikzpicture}
\caption[Visualization of the load-balancing system when the agent sends work to queue 2]{Visualization of the load-balancing system when the agent sends work to queue 2, the bold lines represent the flow of works inside the system.}
\end{figure}
```

### Evolution

In this system, given its state is $(S, E)$, the agent decides to send work to queue $a$. One of these $1 + n + n(m - 1) + 1$ events can happen.

- The work of class $C_{a}$ increases by 1. Because we can only send work to one queue, there is only one event of this type and its rate is $\lambda$.
- The work of class $C_{i}$ decreases by 1. Because there are $n$ classes, we have $n$ events of this kind with the rate $\mu_{1, M_{1}}, \dots, \mu_{n, M_{n}}$ respectively (if the number of work of class $C_{i}$ reaches 0, we consider $\mu_{i, M_{i}} = 0$).
- The environment of class $C_{i}$ changes to a different environment other than $m_{i}$ and the rate changing to environment $j$ is $\xi_{i, M_{i}, j}$. Because there are $m - 1$ possible changes for each class and there are $n$ classes, the number of this kind of event is $n(m - 1)$.
- And a special dummy event where nothing changes.

The form of the normalization constant also changes according to the evolution of the system.

$$
C = \lambda + \sum_{i = 1}^{n} \max_{j} \mu_{i, j} + \sum_{i = 1}^{n} \max_{j} \sum_{k = 1, k \neq j}^{m} \xi_{i, j, k} + \epsilon
$$

## System representation

In this internship, we limited ourselves on the cases where $n = 2$ for both systems. A system of that kind can be represented as a grid whose each side represents the evolution of each queue. In this report, we use the vertical side for the first queue, and the horizontal one for the second. Each cell of that grid represent a specific observable state of the system.

```{=latex}
\begin{figure}
\centering
\def\mycolumns{5}
\def\myrows{5}
\begin{tikzpicture}[x=1cm, y=1cm, mynums/.style={inner sep=0}]
\fill[gray!50] (1,1) -- +(0, 1) -- +(1, 1) -- +(1,0) -- cycle;
\draw[step=1cm] (0,0) grid (\mycolumns + 1,\myrows + 1);
\foreach\x in {0,...,\mycolumns}
  \node[anchor=south, mynums] at (\x+0.5, \myrows + 1.25) {\x};
\foreach\x in {0,...,\myrows}
  \node[anchor=east, mynums] at (-0.25, \myrows - \x + 0.5) {\x};
\draw[->]
  (-1, \myrows + 0.5) --
    node[anchor=east,align=center] {$Q_{1}$}
  (-1, 0.5);
\draw[->]
  (0.5, \myrows + 2) --
    node[anchor=south,align=center] {$Q_{2}$}
  (\mycolumns + 0.5, \myrows + 2);
\end{tikzpicture}
\caption[Representation of a system with 2 queues]{Representation of a system with 2 queues and the length limit of both queues are 5, the gray cell represents an observable state of the system $S = (4, 1)$}
\end{figure}
```

# Reinforcement learning

Reinforcement learning is used for training the agent to attain his goal. In this session, we present a short summary and introduce some related notions that could be useful for later.

## Parameters

A simple reinforcement learning is modeled by a markov decision process (MDP) whose parameters include as follow:

- $\mathcal{S}$: the set of all states of the system.
- $\mathcal{A}$: the set of all actions, and $A_{s}$, the set of all actions available from state $s$.
- $P(s_{t + 1} = s' | s_{t} = s, a_{t} = a)$: the probability that action $a$ in state $s$ at time $t$ will lead to state $s'$ at time $t + 1$.
- $R(s' | s, a)$: the immediate reward received after transitioning from state $s$ to state $s'$, due to action $a$.

In addition, we have policy $\pi(a | s) = \mathbb{P}[A_{t} = a | S_{t} = s]$, a distribution over actions given states. In our problem, $\mathcal{S}$ is represented as the grid, $\mathcal{A}$ is activating / sending work to a queue, $P_{a}(s, s')$ is the probability that specified in the evolution section and $R_{a}(s, s')$ is $-C$ where $C$ is the total cost of the system. There is also an unobservable state inside our system, therefore, our problem is a generalization of the MDP, called partial observable markov decision process (POMDP).

## Bellman equation

The accumulate discounted future reward (return) from time $t$ with the discount factor $0 \le \gamma \le 1$ is:
$$
G_{t} = r_{t + 1} + \gamma r_{t + 2} + \gamma^{2} r_{t + 3} + \gamma^{3} r_{t + 4} + \dots
$$

From that, we have state-value function $V_{\pi}(s)$ is the expected return starting from state $s$, and then following policy $\pi$:
```{=latex}
\begin{equation}
V_{\pi}(s) = \mathbb{E}_{\pi}[G_{t} | S_{t} = s]
(\#eq:state-value)
\end{equation}
```

And action-value function $Q_{\pi}(s, a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\pi$:
```{=latex}
\begin{equation}
Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_{t} | S_{t} = s, A_{t} = a]
(\#eq:action-value)
\end{equation}
```

Finally, the bellman equation are obtained by writing \@ref(eq:state-value) and \@ref(eq:action-value) recursively.
```{=latex}
\begin{align}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a | s) Q_{\pi}(s, a) (\#eq:bellman-v-1) \\
Q_{\pi}(s, a) &= \sum_{s' \in \mathcal{S}} P(s' | s, a) (R(s' | s, a) + \gamma V_{\pi}(s')) (\#eq:bellman-q-1)
\end{align}
```

## Optimal policy

The core idea of this problem is to find an optimal policy $\pi_{*}$ that maximizes the expected reward or $V(s)$. Mathematically, we have $\pi > \pi' \text{ if } V_{\pi}(s) > V_{\pi'}(s) \,\, \forall s \in \mathcal{S}$ and we want to find $\pi_{*}$ such that $\pi_{*} \ge \pi \,\, \forall \pi$. We can do that by finding the optimal state-value function $V_{*}(s) = max_{\pi} V_{\pi}(s)$ or action-value function $Q_{*}(s, a) = max_{\pi} Q_{\pi}(s, a)$, once we have the $V_{*}(s)$ and $Q_{*}(s, a)$, we define the optimal policy as follow:
$$
\pi_{*}(s) = \begin{cases}
1 & \text{ if } a = argmax_{a \in \mathcal{A}} Q_{*}(s, a) \\
0 & otherwise
\end{cases}
$$

Plug all together into equation \@ref(eq:bellman-v-1) and \@ref(eq:bellman-q-1), we have:
```{=latex}
\begin{align}
V_{*}(s) &= max_{a} Q_{*}(s, a) = max_{a} \sum_{s' \in \mathcal{S}} P(s' | s, a) (R(s' | s, a) + \gamma V_{*}(s')) (\#eq:bellman-max-v) \\
Q_{*}(s, a) &= \sum_{s' \in \mathcal{S}} P(s' | s, a) (R(s' | s, a) + \gamma V_{\pi}(s')) = \sum_{s' \in \mathcal{S}} P(s' | s, a) (R(s' | s, a) + max_{a} Q_{*}(s, a)) (\#eq:bellman-max-q)
\end{align}
```

We then move to next sections where we discuss about several training methods based on these equations.
