---
classoption: a4paper, xcolor = usenames,dvipsnames
geometry: margin=2.5cm
bibliography: ref.bib
link-citations: true
biblio-style: alphabetic
biblatexoptions: 
  - backend=biber
output:
  bookdown::pdf_document2:
    papersize: a4
    fig_caption: true
    highlight: tango
    keep_tex: true
    number_sections: true
    pandoc_args:
      - "--listings"
    toc_depth: 3
    toc: false
    latex_engine: lualatex
    citation_package: biblatex
    includes:
      in_header: preamble.tex
      before_body: cover.tex
---

```{r, chunk, include=F, cache=F}
knitr::read_chunk("report.r")
```

```{r init, include=F, cache=F}
```

# Introduction

## Institut de Recherche en Informatique de Toulouse (IRIT)

### About the institut

The Institut de Recherche en Informatique de Toulouse (IRIT), created in 1990, is a Joint Research Unit (UMR 5505) of the Centre National de la Recherche Scientifique (CNRS), the Institut National Polytechnique de Toulouse (INP), the Université Paul Sabatier Toulouse3 (UT3), the Université Toulouse1 Capitole (UT1) and the Université de Toulouse Jean Jaurès (UT2J).

IRIT is one of the largest UMR at the national level, is one of the pillars of research in Occitanie with its 600 members, permanent and non-permanent, and about 100 external collaborators. The laboratory constitutes one of the structuring forces of the IT landscape and its applications in the digital world, both at regional and national level.

IRIT has focused its research on five major scientific issues and six strategic application areas.

- Health, Autonomy, Living, Well-being
- Smart City
- Aerospace and Transportation
- Social Media, Digital Social Ecosystems
- e-Education for learning and teaching
- Heritage and People Safety

As well as strategic action:

- Scientific Computing, Big Data and AI

### Organization

The 24 research groups of the laboratory are dispatched in seven scientific departments:

- Dpt ASR : Architecture, Systems, Networks
- Dpt CISO : HPC, Simulation, Optimization
- Dpt FSL : Reliability of Systems and Software
- Dpt GD : Data Management
- Dpt ICI : Interaction, Collective Intelligence
- Dpt IA : Artificial Intelligence
- Dpt SI : Signals, Images

## The internship

This is the internship description: *"Markov decisions processes (MDPs) and their model free counterpart in reinforcement learning (RL) have known a large success in the last two decades. Although research in these two areas has been taking place for more than fifty years, the field gained momentum only recently following the advent of powerful hardware and algorithms with which supra­human performance were obtained in games like Chess or Go. However, these impressive successes often rely on quite exceptional hardware possibilities and cannot be applied in many usual contexts, where, for instance, the volume of data available or the amount of computing power is more restricted. To define the next generation of more democratic and widely applicable algorithms, such methods still need to deal with very demanding exploration issues as soon as the state/action spaces are not small. One way around this is to use underlying knowledge and structure present in many MDPs. This is especially true for problems related to scheduling and resources sharing in among others server farms, clouds, and cellular wireless networks. The internships will revolve around this theme of improving the efficiency of learning algorithms by leveraging the structure of the underlying problem and focus mainly on model ­free approach."*

# System settings

In this internship, we are interested in learning an efficient policy for some dynamic systems where the internal settings (transitions rate) of the system depend on some environments (or which environment the system is in). We could take one decision at a specific time (doing some things or even doing nothing) to make the system run more efficiently (faster or cheaper depends on each type of problem). However, we can only observe the state of the system (for example: the number of remaining tasks) and not the environments when taking decisions. And therefore, we will explorer Q-learning and derived techniques since they does not need to know the internal settings of the systems. In this report, we focus ourselves on two following system models.

## Queuing system

### Parameters

We have $n$ classes of queue $Q_{1}, \dots, Q_{n}$, and $L_{1}, \dots, L_{n}$ the maximum number of jobs (limit) on each class of queue. For each $Q_{i}$, its behavior is fully controlled by which environment it is in. An environment can be in one of the states $m_{1}, \dots, m_{m}$. And the environment of class $Q_{i}$ is a random variable, denoted by $M_{i}$, which is in one of the states $m_{1}, \dots, m_{m}$. For each environment $m_{j}$, $Q_{i}$ has their own holding cost $c_{i, j}$ (the cost of one unfinished job on the queue), arrival rate $\lambda_{i, j}$ (the rate of one more job arriving to the queue) and departure rate $\mu_{i, j}$ (the rate that a job departs the system (if served)). Furthermore, the environment $M_{i}$ can change from $m_{j}$ and $m_{k}$ with the rate of $\xi_{i, j, k}$. A table summarizing all parameters is shown below.

```{=latex}
\begin{table}[ht]
\caption{Queuing system parameters}
\begin{center}
\begin{tabular}{c c c c c}
    \hline
    \multicolumn{2}{c}{} & $m_{1}$ & $\dots$ & $m_{m}$ \\
    \cline{2-5}
    \multirow{3}{*}{$Q_{1}$} &  Holding cost & $c_{1, 1}$ & $\dots$ & $c_{1, m}$ \\
    & Arrival rate & $\lambda_{1, 1}$ & $\dots$ & $\lambda_{1, m}$ \\
    & Departure rate & $\mu_{1, 1}$ & $\dots$ & $\mu_{1, m}$ \\
    \cline{2-5}
    $\vdots$  \\
    \cline{2-5}
    \multirow{3}{*}{$Q_{n}$} &  Holding cost & $c_{n, 1}$ & $\dots$ & $c_{n, m}$ \\
    & Arrival rate & $\lambda_{n, 1}$ & $\dots$ & $\lambda_{n, m}$ \\
    & Departure rate & $\mu_{n, 1}$ & $\dots$ & $\mu_{n, m}$ \\
    \hline
\end{tabular}
\end{center}
\label{tab:qs-param}
\end{table}
```

```{=latex}
\begin{table}[ht]
\caption{Matrix transition of the environment of $Q_{i}$}
\begin{center}
\begin{tabular}{c c c c}
    \hline
    & $m_{1}$ & $\dots$ & $m_{m}$ \\
    $m_{1}$ & $\xi_{i, 1, 1}$ & \dots & $\xi_{i, 1, m}$ \\
    \vdots \\
    $m_{m}$ & $\xi_{i, m, 1}$ & \dots & $\xi_{i, m, m}$ \\
    \hline
\end{tabular}
\end{center}
\label{tab:mat-transition-ci}
\end{table}
```

The state of the system is represented by two vectors:

- The state $S = (X_{1}, \dots, X_{n})$ where $X_{i}$ is a random variable represents the current number of class $Q_{i}$ jobs and is observable.
- The environment vector $E = (M_{1}, \dots, M_{n})$ and this vector is not observable.

### Cost

The cost of the system is a function of $S$ and $E$. We propose two functions of cost. The first one is a simple linear function.

$$
f_{1}(S, E) = \sum_{1}^{n} c_{i, M_{i}} X_{i}
$$

And the second one is a convex function which is specialized for the case $n = 2$, where $\epsilon$ is a fixed positive constant.

$$
f_{2}(S, E) = c_{1, M_{1}} X_{1} + c_{2, M_{2}} (\epsilon X_{2}^{2} + X_{2})
$$

### Agent

The agent will decide which queues should be activated. His goal is to minimize the cost of the whole system and the only information provided to him is the observable state $S$. Only jobs on activating queues can be processed and finished. In more generic problems, the agent is allowed to activate / deactivate multiple queues at the same time based on some conditions. However, in this internship, the agent can only activate one queue at a time.

```{=latex}
\begin{figure}
\centering
\begin{tikzpicture}[>=latex]
% the shapes
\path
  (0,3cm) pic {queuei=1}
  (0,1cm) pic {queuei=2}
  (0,-2cm) pic {queuei=n};
\path
  (1,4cm) coordinate (aux1)
  (1,-3.5cm) coordinate (aux2)
  (-0.5,0cm) coordinate (aux3)
  (2.5,0cm) coordinate (aux4);
\node[draw,dashed,text width=2.5cm,fit={(aux1) (aux2) (aux3) (aux4)}] (dashed) {};
\node[draw,align=center,circle,inner sep=2pt]
  at (4,-1) (agent)
  {Agent};

%the arrows
\draw[->, line width=1.25]
  ([xshift=-3.5cm]q-1.west) --
    node[anchor=south,align=center] {$\lambda_{1, M_{1}}$}
  (q-1.west);
\draw[->]
  ([xshift=1.5cm]q-1.east) --
    node[anchor=south,align=center] {$\mu_{1, M_{1}}$}
  ([xshift=4.5cm]q-1.east);
\draw[->, line width=1.25]
  ([xshift=-3.5cm]q-2.west) --
    node[anchor=south,align=center] {$\lambda_{2, M_{2}}$}
  (q-2.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-2.east) --
    node[anchor=south,align=center] {$\mu_{2, M_{2}}$}
  ([xshift=4.5cm]q-2.east);
\draw[->, line width=1.25]
  ([xshift=-3.5cm]q-n.west) --
    node[anchor=south,align=center] {$\lambda_{n, M_{n}}$}
  (q-n.west);
\draw[->]
  ([xshift=1.5cm]q-n.east) --
    node[anchor=south,align=center] {$\mu_{n, M_{n}}$}
  ([xshift=4.5cm]q-n.east);
\draw[->,dotted]
  (agent.west) --
    node[anchor=south,align=center] {}
  ([xshift=1.5cm]q-1.east);
\draw[->,densely dotted, line width=1]
  (agent.west) --
    node[anchor=south,align=center] {}
  ([xshift=1.5cm]q-2.east);
\draw[->,dotted]
  (agent.west) --
    node[anchor=south,align=center] {}
  ([xshift=1.5cm]q-n.east);
\path ([xshift=0.5cm]q-2.south) -- ([xshift=0.5cm]q-n.north) node [font=\LARGE, midway, sloped] {$\dots$};
\end{tikzpicture}
\caption[Visualization of the queueing system when the agent activates queue 2]{Visualization of the queueing system when the agent activates queue 2, the bold lines represent the flow of jobs inside the system.}
\end{figure}
```

### Evolution

In the continuos-time scale, each type of event (job arrival, job departure and environment changing) happens independently. That system is quite hard to program, therefore, we used a technique called "uniformization" to move from the continuos-time scale to discrete-time scale. In this time scale, at a given time, only one event can take place, regardless of their type.

In particular, given the system state is $(S, E)$, the agent decides to activate queue $a$. One of these $n + 1 + n(m - 1) + 1$ events can happen.

- The job of class $Q_{i}$ increases by 1. Because there are $n$ classes, we have $n$ events of this kind with the rate $\lambda_{1, M_{1}}, \dots, \lambda_{n, M_{n}}$ respectively (if the number of job of class $Q_{i}$ reaches an upper limit, we consider $\lambda_{i, M_{i}} = 0$).
- The job of class $Q_{a}$ decreases by 1. Because we can only activate only one class, there is only one event of this type and its rate is $\mu_{a, M_{a}}$.
- The environment of class $Q_{i}$ changes to a different environment other than $m_{i}$ and the rate changing to environment $j$ is $\xi_{i, M_{i}, j}$. Because there are $m - 1$ possible changes for each class and there are $n$ classes, the number of this kind of event is $n(m - 1)$.
- And a special dummy event where nothing changes.

A discrete probability distribution is used to express that. In order to satisfy the condition of a probability distribution, all the rates above are divided by a normalization constant $C$ to make sure that their sum are not greater than 1. If that sum is smaller than 1, the special dummy event is used to fill the gap so that the final sum will be equal to 1.

The normalization constant has the form as follow, which is deduced from the above evolution of the system.

$$
C = \sum_{i = 1}^{n} \max_{j} \lambda_{i, j} + \max_{i, j} \mu_{i, j} + \sum_{i = 1}^{n} \max_{j} \sum_{k = 1, k \neq j}^{m} \xi_{i, j, k} + \epsilon
$$

If $\epsilon > 0$, the probability of the dummy event will always be greater than 0. In this internship, as we do not want the system to evolve too slowly, we choose $\epsilon \coloneq 0$.

After obtaining all the information above, we use that discrete probability distribution to obtain the next transition $T$ of the system and denote $S'$ and $E'$ the next state of the system.

## Load-balancing system

### Parameters

For the load-balancing system, we only have one stream of job arrivals with rate $\lambda$ and $n$ queues. Same as above, The queues capacities (holding cost and departure rate) also depend on which environment they are in. Therefore, we have similar parameters to the one of the queuing system.  The only difference in this system is that instead of having multiple arrival rate that depends on the class of queue as well as the environment, we have only one global arrival rate $\lambda$. A table summarizing the parameters of this system is shown below.

```{=latex}
\begin{table}[ht]
\caption{Load-balancing system parameters}
\begin{center}
\begin{tabular}{c c c c c}
    \hline
    \multicolumn{2}{c}{} & $m_{1}$ & $\dots$ & $m_{m}$ \\
    \cline{2-5}
    \multirow{2}{*}{$Q_{1}$} &  Holding cost & $c_{1, 1}$ & $\dots$ & $c_{1, m}$ \\
    & Departure rate & $\mu_{1, 1}$ & $\dots$ & $\mu_{1, m}$ \\
    \cline{2-5}
    $\vdots$  \\
    \cline{2-5}
    \multirow{2}{*}{$Q_{n}$} &  Holding cost & $c_{n, 1}$ & $\dots$ & $c_{n, m}$ \\
    & Departure rate & $\mu_{n, 1}$ & $\dots$ & $\mu_{n, m}$ \\
    \cline{2-5}
    & Global arrival rate & & $\lambda$ & \\
    \hline
\end{tabular}
\end{center}
\label{tab:lbs-param}
\end{table}
```

### Cost

Same as above, the cost of the system is a function of $S$ and $E$. However, in this system, only the simple linear version is used.

$$
f_{1}(S, E) = \sum_{1}^{n} c_{i, M_{i}} X_{i}
$$

### Agent

The goal of the agent is the same: minimizing the cost of the whole system. He also receive the same information: the observable state $S$. The difference here is instead of having to deactivating some queues, all queues now run continuously. His mission is to choose which queue to send the new arriving job to. Only the chosen queue will have new job arriving, the other queues only have to process the remaining jobs of them.

```{=latex}
\begin{figure}
\centering
\begin{tikzpicture}[>=latex]
% the shapes
\path
  (0,3cm) pic {queuei=1}
  (0,1cm) pic {queuei=2}
  (0,-2cm) pic {queuei=n};
\path
  (1,4cm) coordinate (aux1)
  (1,-3.5cm) coordinate (aux2)
  (-0.5,0cm) coordinate (aux3)
  (2.5,0cm) coordinate (aux4);
\node[draw,dashed,text width=2.5cm,fit={(aux1) (aux2) (aux3) (aux4)}] (dashed) {};
\node[draw,align=center,circle,inner sep=2pt]
  at (-2,0) (agent)
  {Agent};

%the arrows
\draw[->, line width=1.25]
  ([xshift=-1cm]agent.west) --
    node[anchor=south,align=center] {$\lambda$}
  (agent.west);
\draw[->, dotted]
  (agent.east) --
    node[anchor=south,align=center] {}
  (q-1.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-1.east) --
    node[anchor=south,align=center] {$\mu_{1, M_{1}}$}
  ([xshift=4.5cm]q-1.east);
\draw[->, densely dotted, line width=1]
  (agent.east) --
    node[anchor=south,align=center] {}
  (q-2.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-2.east) --
    node[anchor=south,align=center] {$\mu_{2, M_{2}}$}
  ([xshift=4.5cm]q-2.east);
\draw[->, dotted]
  (agent.east) --
    node[anchor=south,align=center] {}
  (q-n.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-n.east) --
    node[anchor=south,align=center] {$\mu_{n, M_{n}}$}
  ([xshift=4.5cm]q-n.east);
\path ([xshift=0.5cm]q-2.south) -- ([xshift=0.5cm]q-n.north) node [font=\LARGE, midway, sloped] {$\dots$};
\end{tikzpicture}
\caption[Visualization of the load-balancing system when the agent sends job to queue 2]{Visualization of the load-balancing system when the agent sends job to queue 2, the bold lines represent the flow of jobs inside the system.}
\end{figure}
```

### Evolution

In this system, given its state is $(S, E)$, the agent decides to send job to queue $a$. One of these $1 + n + n(m - 1) + 1$ events can happen.

- The job of class $Q_{a}$ increases by 1. Because we can only send job to one queue, there is only one event of this type and its rate is $\lambda$.
- The job of class $Q_{i}$ decreases by 1. Because there are $n$ classes, we have $n$ events of this kind with the rate $\mu_{1, M_{1}}, \dots, \mu_{n, M_{n}}$ respectively (if the number of job of class $Q_{i}$ reaches 0, we consider $\mu_{i, M_{i}} = 0$).
- The environment of class $Q_{i}$ changes to a different environment other than $m_{i}$ and the rate changing to environment $j$ is $\xi_{i, M_{i}, j}$. Because there are $m - 1$ possible changes for each class and there are $n$ classes, the number of this kind of event is $n(m - 1)$.
- And a special dummy event where nothing changes.

The form of the normalization constant also changes according to the evolution of the system.

$$
C = \lambda + \sum_{i = 1}^{n} \max_{j} \mu_{i, j} + \sum_{i = 1}^{n} \max_{j} \sum_{k = 1, k \neq j}^{m} \xi_{i, j, k} + \epsilon
$$

## System representation

In this internship, we limited ourselves on the cases where $n = 2$ for both systems. A system of that kind can be represented as a grid where each side represents the evolution of each queue. In this report, we use the vertical side for the first queue, and the horizontal one for the second. Each cell of that grid represent a specific observable state of the system.

```{=latex}
\begin{figure}
\centering
\def\mycolumns{5}
\def\myrows{5}
\begin{tikzpicture}[x=1cm, y=1cm, mynums/.style={inner sep=0}]
\fill[gray!50] (1,1) -- +(0, 1) -- +(1, 1) -- +(1,0) -- cycle;
\draw[step=1cm] (0,0) grid (\mycolumns + 1,\myrows + 1);
\foreach\x in {0,...,\mycolumns}
  \node[anchor=south, mynums] at (\x+0.5, \myrows + 1.25) {\x};
\foreach\x in {0,...,\myrows}
  \node[anchor=east, mynums] at (-0.25, \myrows - \x + 0.5) {\x};
\draw[->]
  (-1, \myrows + 0.5) --
    node[anchor=east,align=center] {$Q_{1}$}
  (-1, 0.5);
\draw[->]
  (0.5, \myrows + 2) --
    node[anchor=south,align=center] {$Q_{2}$}
  (\mycolumns + 0.5, \myrows + 2);
\end{tikzpicture}
\caption[Representation of a system with 2 queues]{Representation of a system with 2 queues and the length limit of both queues are 5, the gray cell represents an observable state of the system $S = (4, 1).$}
\end{figure}
```

# Reinforcement learning

Reinforcement learning is used for training the agent to attain his goal. In this session, we present a short summary and introduce some related notions that could be useful for later.

## Parameters

A simple reinforcement learning is modeled by a markov decision process (MDP) whose parameters include as follow:

- $\mathcal{S}$: the set of all states of the system.
- $\mathcal{A}$: the set of all actions, and $A_{S}$, the set of all actions available from state $S$.
- $P(S_{t + 1} = S' | S_{t} = S, a_{t} = a)$: the probability that action $a$ in state $S$ at time $t$ will lead to state $S'$ at time $t + 1$.
- $R(S' | S, a)$: the immediate reward received after transitioning from state $S$ to state $S'$, due to action $a$.

In addition, we have policy $\pi(a | S) = \mathbb{P}[A_{t} = a | S_{t} = S]$, a distribution over actions given states. In our problem, there is also an unobservable state inside our system, therefore, our problem is no longer an MDP, but instead a partial observable markov decision process (POMDP). We will now continue discussing about the MDP and return to the POMDP in later section.

## Bellman equation

First, we want to try the solving method of a MDP.

The accumulate discounted future reward (return) from time $t$ with the discount factor $0 \le \gamma \le 1$ is:
$$
G_{t} = r_{t + 1} + \gamma r_{t + 2} + \gamma^{2} r_{t + 3} + \gamma^{3} r_{t + 4} + \dots = r_{t + 1} + \gamma G_{t + 1}
$$

From that, we have state-value function $V_{\pi}(S)$ is the expected return starting from state $S$, and then following policy $\pi$:

```{=latex}
\begin{equation}
V_{\pi}(S) = \mathbb{E}_{\pi}[G_{t} | S_{t} = S]
(\#eq:state-value)
\end{equation}
```

And action-value function $Q_{\pi}(S, a)$ is the expected return starting from state $S$, taking action $a$, and then following policy $\pi$:

```{=latex}
\begin{equation}
Q_{\pi}(S, a) = \mathbb{E}_{\pi}[G_{t} | S_{t} = S, a_{t} = a]
(\#eq:action-value)
\end{equation}
```

From the fact that $\pi(a | S)$ is a distribution over $\mathcal{A}$ given $S$, we have:

```{=latex}
\begin{equation}
\begin{split}
V_{\pi}(S) {}&= \mathbb{E}_{\pi}[G_{t} | S_{t} = S] \\
  &= \sum_{a \in \mathcal{A}} \pi(a | S) \mathbb{E}_{\pi}[G_{t} | S_{t} = S, a_{t} = a] \\
  &= \sum_{a \in \mathcal{A}} \pi(a | S) Q_{\pi}(S, a)
(\#eq:v-to-q)
\end{split}
\end{equation}
```

Furthermore, we see that the expected future return of $Q_{\pi}(S, a)$ is the sum of the current reward $R(S)$ as well as the expected return of the next state regardless action multiply by the probability of moving to that state. Therefore, we have the equation below:

```{=latex}
\begin{equation}
Q_{\pi}(S, a) = R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{\pi}(S')
(\#eq:q-to-v)
\end{equation}
```

Finally, the Bellman equations are obtained from \@ref(eq:v-to-q) and \@ref(eq:q-to-v):

```{=latex}
\begin{align}
\begin{split}
V_{\pi}(S) {}&= \sum_{a \in \mathcal{A}} \pi(a | S) Q_{\pi}(S, a) \\
  &= \sum_{a \in \mathcal{A}} \pi(a | S) (R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{\pi}(S')) \\
  &= R(S) + \gamma \sum_{a \in \mathcal{A}} \pi(a | S) \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{\pi}(S')
(\#eq:bellman-v-1)
\end{split} \\
\begin{split}
Q_{\pi}(S, a) {}&= R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{\pi}(S') \\
  &= R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) \sum_{a' \in \mathcal{A}} \pi(a' | S') Q_{\pi}(S', a')
(\#eq:bellman-q-1)
\end{split}
\end{align}
```

## Optimal policy for MDP

The core idea of this problem is to find an optimal policy $\pi_{*}$ that maximizes the expected reward or $V(S)$. Mathematically, we have
$$
\pi > \pi' \text{ if } V_{\pi}(S) > V_{\pi'}(S) \,\, \forall S \in \mathcal{S}
$$

And we want to find
$$
\pi_{*} \text{ such that } \pi_{*} \ge \pi \,\, \forall \pi
$$

We can do that by finding the optimal state-value function $V_{*}(S) = \max_{\pi} V_{\pi}(S)$ or action-value function $Q_{*}(S, a) = \max_{\pi} Q_{\pi}(S, a)$ and define the optimal policy as follow:
$$
\pi_{*}(S) = \begin{cases}
1 & \text{ if } a = \arg\max_{a \in \mathcal{A}} Q_{*}(S, a) \\
0 & otherwise
\end{cases}
$$

Plugging all together into equation \@ref(eq:bellman-v-1) and \@ref(eq:bellman-q-1), we have:

```{=latex}
\begin{align}
V_{*}(S) &= R(S) + \max_{a \in \mathcal{A}} \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{*}(S')
(\#eq:bellman-max-v) \\
Q_{*}(S, a) &= R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) \max_{a' \in \mathcal{A}} Q_{*}(S', a')
(\#eq:bellman-max-q)
\end{align}
```

## Efficient policy when having unobservable environment

Our problem is a POMDP, the parameters could be seen as follow:

- $\mathcal{S}$: the set of observable states is represented as the grid.
- $\mathcal{E}$: the set of unobservable environment states.
- $\mathcal{A}$: the action of activating / sending work to a queue.
- $P(S', E' | S, E, a)$: the transition probability that is clarified in the evolution section and depends on the both visible and hidden state (S, E).
- $R(S' | S, E, a)$: the immediate reward is $-C$ where $C$ is the total holding cost of the system. Since this function depends only on the the state (S, E) of system, we could write $R(S, E)$ instead.

Since the goal of the internship is not about solving the POMDP, in the later sections, we will explore how learning algorithms of MDP work in the context of unobservable environment.

# Online Q-learning

## About the algorithm

Online Q-learning is probably one of the most simple and popular algorithm for reinforcement learning problem. It is used to estimate $Q_{*}(S, a) \,\, \forall S \in \mathcal{S} \text{ and } a \in \mathcal{A}$ while interacting with the system. This algorithm only requires the real system or a simulator to interact with, and not the internal settings of that system, which makes it fit the context of this internship and more broadly real-life for a MDP, where these settings can change at any time. Beside its simplicity and close to real life context, the algorithm is also proven to converge almost surely when the number of visits to each state goes to $\infty$ in [@q-learning-converge]. Visually, we can imagine that our agent will follow a grid trajectory of states and try to discover what is the action-value at that state with a specific action.

```{=latex}
\begin{figure}
\centering
\def\mycolumns{5}
\def\myrows{5}
\begin{tikzpicture}[x=1cm, y=1cm, mynums/.style={inner sep=0}]
\fill[gray!50] (0,5) -- +(0, 1) -- +(1, 1) -- +(1,0) -- cycle;
\draw[step=1cm] (0,0) grid (\mycolumns + 1,\myrows + 1);
\foreach\x in {0,...,\mycolumns}
  \node[anchor=south, mynums] at (\x+0.5, \myrows + 1.25) {\x};
\foreach\x in {0,...,\myrows}
  \node[anchor=east, mynums] at (-0.25, \myrows - \x + 0.5) {\x};
\draw[->]
  (-1, \myrows + 0.5) --
    node[anchor=east,align=center] {$Q_{1}$}
  (-1, 0.5);
\draw[->]
  (0.5, \myrows + 2) --
    node[anchor=south,align=center] {$Q_{2}$}
  (\mycolumns + 0.5, \myrows + 2);
\draw[->, line width=1.25] (0.5, 5.5) -- (1.5, 5.5) -- (1.5, 3.5) -- (0.5, 3.5) -- (0.5, 2.5) -- (3.5, 2.5);
\end{tikzpicture}
\caption[An example of the path which the agent might take]{An example of the path which the agent might take. It starts from $(0, 0)$ and moves to $(3, 3)$. The next state could be either $(3, 4), (4, 3), (3, 2), (2, 3)$ depends on the action it chooses and the transition probabilities.}
\end{figure}
```

In this report, we want to investigate the Q-learning algorithm when applied to our POMDP. We used the following algorithm:

```{=latex}
\begin{algorithm}
\caption{Online Q-Learning}\label{alg:on-q-learning}
\KwIn{$T > 0$ number iterations, $\gamma$ discount factor, $\epsilon$ exploration factor, $\eta$ learning rate}
\KwOut{$Q_{*}$, $\pi_{*}$}
\For {$t \gets 0$ \KwTo $T$} {
  $S, E \gets \mathfrak{S}$ current state of the system\;
  $a \gets 
    \begin{cases}
    \begin{aligned}
      \text{one random possible action}&, && \text{ with the probability of } \epsilon \text{ (exploration)}\\ 
      \arg\max_{a \in \mathcal{A}} Q_{t}(S, a)&, && \text{ with the probability of } 1 - \epsilon \text{ (learning)}
    \end{aligned}
    \end{cases}
  $\\
  \tcc{$S' = S$ if $E' \neq E$ and vice versa}
  $R, S', E' \gets \mathfrak{S}(a, S, E)$\;
  $Q_{t+1}(S, a) \gets Q_{t}(S, a) + \eta (R + \gamma \max_{a' \in \mathcal{A}} Q_{t}(S', a') - Q_{t}(S, a))$ \;
}
\end{algorithm}
```

## Implementation

### From Python to C++

In this section, we document some difficulties while implementing the framework and how we overcome it. For illustration purpose, we will test the algorithm with a very simple problem of the [queuing system](#queuing-system)
and linear cost function with no environment (or constant environment). The parameter of this system is show in table \@ref(tab:q-learning-test).

```{=latex}
\begin{table}[ht]
\caption{Simple queuing problem for testing}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $2$ \\
    & Arrival rate $\lambda_{1}$ & $0.135$ \\
    & Departure rate $\mu_{1}$ & $0.3$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $3$ \\
    & Arrival rate $\lambda_{2}$ & $0.135$ \\
    & Departure rate $\mu_{2}$ & $0.3$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:q-learning-test}
\end{table}
```

It has been proven that if we have $c_{1} \mu_{1} \ge c_{2} \mu_{2}$, the optimal policy for all cases is activating the first queue if possible, and vice versa in the case where $c_{1} \mu_{1} \le c_{2} \mu_{2}$. From table \@ref(tab:q-learning-test), we know that our optimal action here is activating the second queue.

With a naive Python implementation, the algorithm works correctly when the limit of both queue are small.

```{r, 3x3-python, resize.width="0.5\\textwidth", resize.height="!", fig.align="center", fig.cap="Result of online Q-learning using Python for 3x3 case. $\\gamma = 0.999$, $\\epsilon = 0.99$"}
```

First, we see that on the first column where $Q_{2} = 0$, our agent always chooses to activate the first queue, because this is the only option possible, because there is no job on the second queue. Same thing holds for the first row where $Q_{1} = 0$. For the other cells, we have the black color which means the agent chooses the second queue which is aligned with the analytical solution as above.

This implementation runs $10^6$ iterations for approximately $2$ minutes. This is quite good for number with small case. However, if we increase the limit to 10, with the same number of iterations gives a imperfect result.

```{r, 10x10-python, fig.show="hold", resize.width="0.5\\textwidth", resize.height="!", fig.align="center", fig.cap="Result of online Q-learning using Python for 10x10 case. $\\gamma = 0.99$, $\\epsilon = 0.999$", fig.subcap=c("For $10^6$ iterations", "For $10^7$ iterations")}
```

In the figure above, we can see that even after 20 minutes with $10^7$ iterations, the algorithm still does not converge completely yet. To have a clear view about the reason ưhy, we have figure \@ref(fig:10x10-python-n-visit).

```{r, 10x10-python-n-visit, fig.show="hold", resize.width="0.5\\textwidth", resize.height="!", fig.align="center", fig.cap="Number of visit for the system above. $\\gamma = 0.99$, $\\epsilon = 0.999$", fig.subcap=c("Activating $Q_{1}$", "Activating $Q_{2}$")}
```

We can see that our agent is stuck at $(0, 0)$, only the area around the origin has a high number of visits. The rest have relatively low number of visits and therefore, does not converge because of the nonfulfillment of the condition in [@q-learning-converge].

To speed up the algorithm, we decided to switch to C++, which is famous for its speed and its mature support for high-speed computation with library like `Eigen` and then expose a Python interface for more ease of use. This is also the same approach for many scientific libraries in Python such as `Numpy` or  `Tensorflow`. After switching to C++ and some improvements later, the algorithm now can run $10^9$ iterations in only 1 minute, a $2 \times 10^3$ speed up.

### From $n$-dimensions to $2$-dimensions

With C++, the simulator is already running faster. However, there are still room for improvements. Now, the probability $P(S', E' | S, E, a)$ is calculated on-the-fly when we meet that state $(S, E)$ while running the algorithm, we could make it faster by calculate the matrix transition for all the states beforehand. However, the state $S$ and $E$ of the system are each represented by a vector of $n$ entries, if we keep using vector like that, we will need a tensor of $n + n + 1 = 2N + 1$ dimensions (first $n$ dimensions for saving the observable state $S$, the next $n$ dimensions for saving the unobservable state $S$ and the last one for action). In addition, since there is no sparse version for high-dimensional tensor, it will take a lot of memory to save that tensor, more precisely:

$$
(L_{1} + 1) \times \dots \times (L_{n} + 1) \times \underbrace{n^{m}}_{n \text{ queues and } m \text{ environments}} \times \underbrace{n}_{n \text{ actions}}
$$

If we try to allocate a tensor of that size, we will get an "out of memory" error.

Furthermore, in other algorithms, we are required to iterate all over the set of states. If $n$, the number of queues is static, we could do something similar to algorithm \@ref(alg:states-3). However our goal is to let $n$ as a input for the program. So we can not use that algorithm there.

```{=latex}
\begin{algorithm}
\caption{Iteration over the set of visible states $\mathcal{S}$ when $n = 3$} \label{alg:states-3}
\For {$i \gets 0$ \KwTo $L_{1}$} {
  \For {$j \gets 0$ \KwTo $L_{2}$} {
    \For {$k \gets 0$ \KwTo $L_{3}$} {
      $S \gets (i, j, k)$\;
      \dots\;
    }
  }
}
\end{algorithm}
```

In the end, we decide to encode the whole state into an integer by extending the notion of base of number. Given state $(S, E)$, we introduce 3 functions:
$$
\begin{aligned}
f_{\mathcal{S}}(S) &= f(X_{1}, \dots, X_{n}) = X_{1} L_{2} \dots L_{n} + X_{2} L_{3} \dots L_{n} + \dots + X_{n - 1} L_{n} + X_{n} \\
f_{\mathcal{E}}(E) &= f(M_{1}, \dots, M_{n}) = M_{1} m^{n - 1} + M_{2} m^{n - 2} + \dots + M_{n - 1} m + M_{n} \\
f_{\mathcal{S, E}(S, E)} &= f(X_{1}, \dots, X_{n}, M_{1}, \dots, M_{n}) = m^{n} f_{\mathcal{S}}(S) + f_{\mathcal{E}}(E)
\end{aligned}
$$

All three functions are one-to-one mapping, so we can replace the state with the result of these function without losing anything. Because of doing so, we have achieved several benefits:

- We can calculate the transition matrix right now and use a sparse matrix to store it for efficiency in both storage and performance (since the transition matrix will be full of 0).
- Turn a problem with unknown dimension to a 2-dimensional one which makes accessing and looping through the states easier.
- Overall speed improvement since we only need to deal with integer instead of a vector.

# Offline Q-learning

## About the algorithm

In this version of Q-learning, we do not explore or follow the trajectory as in the online version, but we will force the calculation of the Q-value for all pairs $(S, a)$. This method is faster than online Q-learning, however, it requires a reliable simulator of the system to be able to work. The algorithm is described below:

```{=latex}
\begin{algorithm}
\caption{Offline Q-Learning}\label{alg:off-q-learning}
\KwIn{$T > 0$ number iterations, $\gamma$ discount factor, $\eta$ learning rate}
\KwOut{$Q_{*}$, $\pi_{*}$}
$\mathfrak{S}$ simulator of the system\;
$E$ initial environment state of the system\;
\For {$t \gets 0$ \KwTo $T$} {
  $ -, -, E' \gets \mathfrak{S}(-, -, E)$\;
  \If{$E \ne E'$}{
    $E \gets E'$ \;
  }
  \For {$S \in \mathcal{S}$} {
    \For {$a \in \mathcal{A}$} {
      $S', R, - \gets \mathfrak{S}(a, S, E)$\;
      $Q_{t+1}(S, a) \gets Q_{t}(S, a) + \eta (R + \gamma \max_{a'} Q_{t}(S', a') - Q_{t}(S, a))$\;
    }
  }
}
\end{algorithm}
```

# Value iteration

## About the algorithm

This algorithm is directly based on equation \@ref(eq:bellman-max-v) and used to estimate the value of $V_{*}(S)$. After obtaining an estimate of $V_{*}(S)$, the policy is deduced as $\pi_{*}(S) = \arg\max_{a} R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{*}(S')$. The algorithm is written as follow:

```{=latex}
\begin{algorithm}
\caption{Value iteration} \label{alg:value-iteration}
\KwIn{$\gamma$ discount factor}
\KwOut{$V_{*}$, $\pi_{*}$}
$\Delta \gets \infty$\;
\While {\Delta > \epsilon} {
  \For {$S \in \mathcal{S}$} {
    $V_{t+1}(S) \gets \max_{a \in \mathcal{A}} R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{*}(S')$\;
  }
  $\Delta \gets \lVert V_{t + 1} - V_{t} \rVert$\_{2};
}
\end{algorithm}
```

We can see above that the algorithm makes use of $P(S' | S, a)$, because of this, this algorithm converge a lot faster than both Q-learning methods but we have to know the transition probability, so this method is use only as a baseline to compare with the result of Q-learning.

If we have only one constant environment $E_{c}$ (or no environment changing), we could rewrite $P(S', E_{c} | S, E_{c}, a) = P(S' | S, a)$ and value iterations work correctly. However, as we have unobservable environment, algorithm \@ref(alg:value-iteration) can not work without modification because we have $P(S', E' | S, E, a)$ but we want to estimate $V_{*}(S)$ and not $V_{*}(S, E)$. In the next section, we discuss about how to get an empirical estimate of $P(S'|S, a)$ based on $P(S', E' | S, E, a)$.

## Empirical transition probability

Our idea is to calculate the probability of the system stay in a particular state $E \in \mathcal{E}$ and then multiply it with the transition probability to get the average probability. First, we have some equations:

```{=latex}
\begin{equation}
  P(S'|S, E, a) = 
    \begin{cases}
        P(S', E|S, E, a) \quad & \text{if} \, S' \neq S \\
        1 - \sum_{S' \neq S} P(S', E|S, E, a) = \sum_{E'} P(S, E'|S, E, a) \quad & \text{otherwise} \\
    \end{cases}
(\#eq:p-s)
\end{equation}
```

```{=latex}
\begin{equation}
  P(E'|S, E, a) = 
    \begin{cases}
        P(S, E'|S, E, a) \quad & \text{if} \, E' \neq E \\
        1 - \sum_{E' \neq E} P(S, E'|S, E, a) \quad & \text{otherwise} \\
    \end{cases}
(\#eq:p-e)
\end{equation}
```

The logic behind equation \@ref(eq:p-s) and \@ref(eq:p-e) is the fact that only one event can happen at a time.

```{=latex}
\begin{equation}
  P(S, E'|S, E, a) = 
    \begin{cases}
        \xi_{i, M_{i}, M_{i}'} \quad & \text{if} \, E' \text{ and } E \text{ differ by only one entry at index } i \\
        0 \quad & \text{else} \\
    \end{cases}
(\#eq:p-e-e)
\end{equation}
```

By rewriting $P(S, E' | S, E, a)$ as in equation \@ref(eq:p-e-e), we see that $P(E'|S, E, a)$ does not depend on $S, a$ and therefore $P(E'|S, E, a) = P(E'| E)$. From that, we could calculate the steady state by an iterative method using the equation below:

```{=latex}
\begin{equation}
\label{eqn:steady_prob}
    \pi_{t + 1}(E') = \sum_{E \in \mathcal{E}} P(E'| E) \pi_{t}(E)
\end{equation}
```

After $\pi(E)$ converges to $\tilde{\pi}(E)$ and using \@ref(eq:p-s), we have the final empirical value we want:

```{=latex}
\begin{equation}
\label{eqn:p_tilde}
    \tilde{P}(S'| S, a) = \sum_{E \in \mathcal{E}} \tilde{\pi}(E) P(S'| S, E, a)
\end{equation}
```

# Result

## No environment changing (MDP)

### Queueing

First we will try with the linear cost. This is the simplest case and since it is proven mathematically (?), it is only a test to show that all algorithms works correctly. We will try two tuples of parameters, the first one will be:

```{=latex}
\begin{table}[ht]
\caption{Simple queuing 1}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $3$ \\
    & Arrival rate $\lambda_{1}$ & $0.13$ \\
    & Departure rate $\mu_{1}$ & $0.3$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $1$ \\
    & Arrival rate $\lambda_{2}$ & $0.13$ \\
    & Departure rate $\mu_{2}$ & $0.3$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:simple-queuing-1}
\end{table}
```

The theoretical optimal action here is $1$.

(ref:title-simple-queuing-1) Result of table \@ref(tab:simple-queuing-1) for 20x20 case. $\gamma = 0.999$, $\epsilon = 0.999$.

```{r, 20x20-simple-queuing-1, fig.show="hold", resize.width="0.33\\textwidth", resize.height="!", fig.align="center", fig.cap="(ref:title-simple-queuing-1)", fig.subcap=c("Online Q-learning", "Offline Q-learning", "Value iterations")}
```

And the second one is nearly the same, except the holding cost is reverse, and therefore the optimal action here is $2$:

```{=latex}
\begin{table}[ht]
\caption{Simple queuing 2}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $1$ \\
    & Arrival rate $\lambda_{1}$ & $0.13$ \\
    & Departure rate $\mu_{1}$ & $0.3$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $3$ \\
    & Arrival rate $\lambda_{2}$ & $0.13$ \\
    & Departure rate $\mu_{2}$ & $0.3$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:simple-queuing-2}
\end{table}
```

(ref:title-simple-queuing-2) Result of table \@ref(tab:simple-queuing-2) for 20x20 case. $\gamma = 0.999$, $\epsilon = 0.999$.

```{r, 20x20-simple-queuing-2, fig.show="hold", resize.width="0.33\\textwidth", resize.height="!", fig.align="center", fig.cap="(ref:title-simple-queuing-2)", fig.subcap=c("Online Q-learning", "Offline Q-learning", "Value iterations")}
```

As we can see, all three algorithms work well for this simple case. We will move to more complex case.

### Load-balancing

For this system, We first try a system where two queues have the same parameters:

```{=latex}
\begin{table}[ht]
\caption{Simple load-balancing 1}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $3$ \\
    & Departure rate $\mu_{1}$ & $0.25$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $3$ \\
    & Departure rate $\mu_{2}$ & $0.25$ \\
    \cline{2-3}
    & Global arrival rate & $0.5$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:simple-lb-1}
\end{table}
```

(ref:title-simple-lb-1) Result of table \@ref(tab:simple-lb-1) for 20x20 case. $\gamma = 0.999$, $\epsilon = 0.999$.

```{r, 20x20-simple-lb-1, fig.show="hold", resize.width="0.33\\textwidth", resize.height="!", fig.align="center", fig.cap="(ref:title-simple-lb-1)", fig.subcap=c("Online Q-learning", "Offline Q-learning", "Value iterations")}
```

Intuitively, if two queues has the same parameters, the optimal action will be sending job to the queue that has less jobs, which is the result of value iterations method (the right grid) of figure \@ref(fig:20x20-simple-lb-1). There is also a gray color indicates that at those states, either action $1$ (sending new job to $Q_{1}$) or $2$ (sending to new job to Q_{2}) leads to the same result, because they have the same number of jobs.

For two Q-learning methods, the online and offline version has been trained for $3 \times 10^{10}$ and $5 \times 10^{7}$ iterations respectively and takes 30 minutes each, however, there is still a clear difference between them. The offline version is nearly converged and is quite similar to the policy of value iterations method. However, the online one just begins to form an uncompleted diagonal. The difference can be explained by the fact that in the offline version, each pair state-action is visited $5 \times 10^{7}$ times, so the total number of iterations will be $2 \times 41^{2} \times 5 \times 10^{7} \approx 1.6 \times 10^{11}$, which is $\gg 3 \times 10^{10}$, the total number of iterations of the online version.

Furthermore, even if we run the online version with that much iterations, it will still not guarantee that we will have a good approximation of the offline result. This is because we are following the trajectory (or the evolution of the system) and therefore it is impossible to visit all states equally. We can see the number of visits of each pair state-action below.

(ref:title-simple-lb-1-n-visit) Number of visits of online Q-learning on table \@ref(tab:simple-lb-1) for 20x20 case.

```{r, 20x20-simple-lb-1-n-visit, fig.show="hold", resize.width="0.5\\textwidth", resize.height="!", fig.align="center", fig.cap="(ref:title-simple-lb-1-n-visit)", fig.subcap=c("Sending to $Q_{1}$", "Sending to $Q_{2}$")}
```

The number of visits is high around $(0, 0)$ and $(40, 40)$. Central region of the grid receives much less visits. Therefore, we see in figure \@ref(fig:20x20-simple-lb-1) that the diagonal is beginning to form from these two points but not the region near the grid center.

\newpage
