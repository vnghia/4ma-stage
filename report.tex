% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper, xcolor = usenames,dvipsnames]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=2.5cm]{geometry}
\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstset{defaultdialect=[5.3]Lua}
\lstset{defaultdialect=[x86masm]Assembler}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{setspace}
\usepackage{float}
\usepackage{fontspec}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[ruled]{algorithm2e}
\usepackage{siunitx}
\usepackage{amsthm}
\newtheorem*{lemma*}{Lemma}
\usetikzlibrary{chains,shapes.multipart}
\usetikzlibrary{shapes,calc,fit}
\usetikzlibrary{automata,positioning}
\floatplacement{figure}{H}
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
  {-2.5ex\@plus -1ex \@minus -.25ex}%
  {1.25ex \@plus .25ex}%
  {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4}
\hypersetup{
  colorlinks = true,
}
\tikzset{
  queuei/.pic={
      \draw[line width=1pt]
      (0,0) -- ++(2cm,0) -- ++(0,-1cm) -- ++(-2cm,0);
      \foreach \Val in {1,...,3}
      \draw ([xshift=-\Val*10pt]2cm,0) -- ++(0,-1cm);
      \node[above] at (1cm,0) {$c_{#1, m_{#1}}$};
      \node[] at (0.5cm,-0.5cm) (q-#1) {};
    },
  mytri/.style={
      draw,
      shape=isosceles triangle,
      isosceles triangle apex angle=60,
      inner xsep=0.65cm
    }
}
\usepackage[style=alphabetic,backend=biber]{biblatex}
\addbibresource{ref.bib}

\author{}
\date{\vspace{-2.5em}}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}

\onehalfspacing

\pagenumbering{gobble}

\vspace*{\fill}
\begin{center}
  \Large{\textbf{Internship report}}\\
  \vspace*{1\baselineskip}
  Vo Van Nghia\\
  \vfill
  \vspace*{\fill}
  \Large{\textbf{Date}}\\
  29 Sep, 2022
\end{center}

\newpage

\newpage
\pagenumbering{roman}
\renewcommand{\contentsname}{Table of contents}
\tableofcontents

\newpage
\pagenumbering{arabic}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

\hypertarget{institut-de-recherche-en-informatique-de-toulouse-irit}{%
\subsection{Institut de Recherche en Informatique de Toulouse (IRIT)}\label{institut-de-recherche-en-informatique-de-toulouse-irit}}

\hypertarget{about-the-institut}{%
\subsubsection{About the institut}\label{about-the-institut}}

The Institut de Recherche en Informatique de Toulouse (IRIT), created in 1990, is a Joint Research Unit (UMR 5505) of the Centre National de la Recherche Scientifique (CNRS), the Institut National Polytechnique de Toulouse (INP), the Université Paul Sabatier Toulouse3 (UT3), the Université Toulouse1 Capitole (UT1) and the Université de Toulouse Jean Jaurès (UT2J).

IRIT is one of the largest UMR at the national level, is one of the pillars of research in Occitanie with its 600 members, permanent and non-permanent, and about 100 external collaborators. The laboratory constitutes one of the structuring forces of the IT landscape and its applications in the digital world, both at regional and national level.

IRIT has focused its research on five major scientific issues and six strategic application areas.

\begin{itemize}
\tightlist
\item
  Health, Autonomy, Living, Well-being
\item
  Smart City
\item
  Aerospace and Transportation
\item
  Social Media, Digital Social Ecosystems
\item
  e-Education for learning and teaching
\item
  Heritage and People Safety
\end{itemize}

As well as strategic action:

\begin{itemize}
\tightlist
\item
  Scientific Computing, Big Data and AI
\end{itemize}

\hypertarget{organization}{%
\subsubsection{Organization}\label{organization}}

The 24 research groups of the laboratory are dispatched in seven scientific departments:

\begin{itemize}
\tightlist
\item
  Dpt ASR : Architecture, Systems, Networks
\item
  Dpt CISO : HPC, Simulation, Optimization
\item
  Dpt FSL : Reliability of Systems and Software
\item
  Dpt GD : Data Management
\item
  Dpt ICI : Interaction, Collective Intelligence
\item
  Dpt IA : Artificial Intelligence
\item
  Dpt SI : Signals, Images
\end{itemize}

\hypertarget{the-internship}{%
\subsection{The internship}\label{the-internship}}

This is the internship description: \emph{``Markov decisions processes (MDPs) and their model free counterpart in reinforcement learning (RL) have known a large success in the last two decades. Although research in these two areas has been taking place for more than fifty years, the field gained momentum only recently following the advent of powerful hardware and algorithms with which supra­human performance were obtained in games like Chess or Go. However, these impressive successes often rely on quite exceptional hardware possibilities and cannot be applied in many usual contexts, where, for instance, the volume of data available or the amount of computing power is more restricted. To define the next generation of more democratic and widely applicable algorithms, such methods still need to deal with very demanding exploration issues as soon as the state/action spaces are not small. One way around this is to use underlying knowledge and structure present in many MDPs. This is especially true for problems related to scheduling and resources sharing in among others server farms, clouds, and cellular wireless networks. The internships will revolve around this theme of improving the efficiency of learning algorithms by leveraging the structure of the underlying problem and focus mainly on model ­free approach.''}

\hypertarget{system-settings}{%
\section{System settings}\label{system-settings}}

In this internship, we are interested in learning an efficient policy for some dynamic systems where the internal settings (transitions rate) of the system depend on some environments (or which environment the system is in). We could take one decision at a specific time (doing some things or even doing nothing) to make the system run more efficiently (faster or cheaper depends on each type of problem). However, we can only observe the state of the system (for example: the number of remaining tasks) and not the environments when taking decisions. And therefore, we will explorer Q-learning and derived techniques since they does not need to know the internal settings of the systems. In this report, we focus ourselves on two following system models.

\hypertarget{queuing-system}{%
\subsection{Queuing system}\label{queuing-system}}

\hypertarget{parameters}{%
\subsubsection{Parameters}\label{parameters}}

We have \(n\) classes of queue \(Q_{1}, \dots, Q_{n}\), and \(L_{1}, \dots, L_{n}\) the maximum number of jobs (limit) on each class of queue. For each \(Q_{i}\), its behavior is fully controlled by which environment it is in. An environment can be in one of the states \(m_{1}, \dots, m_{m}\). And the environment of class \(Q_{i}\) is a random variable, denoted by \(M_{i}\), which is in one of the states \(m_{1}, \dots, m_{m}\). For each environment \(m_{j}\), \(Q_{i}\) has their own holding cost \(c_{i, j}\) (the cost of one unfinished job on the queue), arrival rate \(\lambda_{i, j}\) (the rate of one more job arriving to the queue) and departure rate \(\mu_{i, j}\) (the rate that a job departs the system (if served)). Furthermore, the environment \(M_{i}\) can change from \(m_{j}\) and \(m_{k}\) with the rate of \(\xi_{i, j, k}\). A table summarizing all parameters is shown below.

\begin{table}[!htbp]
\caption{Queuing system parameters}
\begin{center}
\begin{tabular}{c c c c c}
    \hline
    \multicolumn{2}{c}{} & $m_{1}$ & $\dots$ & $m_{m}$ \\
    \cline{2-5}
    \multirow{3}{*}{$Q_{1}$} &  Holding cost & $c_{1, 1}$ & $\dots$ & $c_{1, m}$ \\
    & Arrival rate & $\lambda_{1, 1}$ & $\dots$ & $\lambda_{1, m}$ \\
    & Departure rate & $\mu_{1, 1}$ & $\dots$ & $\mu_{1, m}$ \\
    \cline{2-5}
    $\vdots$  \\
    \cline{2-5}
    \multirow{3}{*}{$Q_{n}$} &  Holding cost & $c_{n, 1}$ & $\dots$ & $c_{n, m}$ \\
    & Arrival rate & $\lambda_{n, 1}$ & $\dots$ & $\lambda_{n, m}$ \\
    & Departure rate & $\mu_{n, 1}$ & $\dots$ & $\mu_{n, m}$ \\
    \hline
\end{tabular}
\end{center}
\label{tab:qs-param}
\end{table}

\begin{table}[!htbp]
\caption{Matrix transition of the environment of $Q_{i}$}
\begin{center}
\begin{tabular}{c c c c}
    \hline
    & $m_{1}$ & $\dots$ & $m_{m}$ \\
    $m_{1}$ & $\xi_{i, 1, 1}$ & \dots & $\xi_{i, 1, m}$ \\
    \vdots \\
    $m_{m}$ & $\xi_{i, m, 1}$ & \dots & $\xi_{i, m, m}$ \\
    \hline
\end{tabular}
\end{center}
\label{tab:mat-transition-ci}
\end{table}

The state of the system is represented by two vectors:

\begin{itemize}
\tightlist
\item
  The state \(S = (X_{1}, \dots, X_{n})\) where \(X_{i}\) is a random variable represents the current number of class \(Q_{i}\) jobs and is observable.
\item
  The environment vector \(E = (M_{1}, \dots, M_{n})\) and this vector is not observable.
\end{itemize}

\hypertarget{cost}{%
\subsubsection{Cost}\label{cost}}

The cost of the system is a function of \(S\) and \(E\). We propose two functions of cost. The first one is a simple linear function.

\[
f_{1}(S, E) = \sum_{1}^{n} c_{i, M_{i}} X_{i}
\]

And the second one is a convex function which is specialized for the case \(n = 2\), where \(\epsilon\) is a fixed positive constant.

\[
f_{2}(S, E) = c_{1, M_{1}} X_{1} + c_{2, M_{2}} (\epsilon X_{2}^{2} + X_{2})
\]

\hypertarget{agent}{%
\subsubsection{Agent}\label{agent}}

The agent will decide which queues should be activated. His goal is to minimize the cost of the whole system and the only information provided to him is the observable state \(S\). Only jobs on activating queues can be processed and finished. In more generic problems, the agent is allowed to activate / deactivate multiple queues at the same time based on some conditions. However, in this internship, the agent can only activate one queue at a time.

\begin{figure}
\centering
\begin{tikzpicture}[>=latex]
% the shapes
\path
  (0,3cm) pic {queuei=1}
  (0,1cm) pic {queuei=2}
  (0,-2cm) pic {queuei=n};
\path
  (1,4cm) coordinate (aux1)
  (1,-3.5cm) coordinate (aux2)
  (-0.5,0cm) coordinate (aux3)
  (2.5,0cm) coordinate (aux4);
\node[draw,dashed,text width=2.5cm,fit={(aux1) (aux2) (aux3) (aux4)}] (dashed) {};
\node[draw,align=center,circle,inner sep=2pt]
  at (4,-1) (agent)
  {Agent};

%the arrows
\draw[->, line width=1.25]
  ([xshift=-3.5cm]q-1.west) --
    node[anchor=south,align=center] {$\lambda_{1, M_{1}}$}
  (q-1.west);
\draw[->]
  ([xshift=1.5cm]q-1.east) --
    node[anchor=south,align=center] {$\mu_{1, M_{1}}$}
  ([xshift=4.5cm]q-1.east);
\draw[->, line width=1.25]
  ([xshift=-3.5cm]q-2.west) --
    node[anchor=south,align=center] {$\lambda_{2, M_{2}}$}
  (q-2.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-2.east) --
    node[anchor=south,align=center] {$\mu_{2, M_{2}}$}
  ([xshift=4.5cm]q-2.east);
\draw[->, line width=1.25]
  ([xshift=-3.5cm]q-n.west) --
    node[anchor=south,align=center] {$\lambda_{n, M_{n}}$}
  (q-n.west);
\draw[->]
  ([xshift=1.5cm]q-n.east) --
    node[anchor=south,align=center] {$\mu_{n, M_{n}}$}
  ([xshift=4.5cm]q-n.east);
\draw[->,dotted]
  (agent.west) --
    node[anchor=south,align=center] {}
  ([xshift=1.5cm]q-1.east);
\draw[->,densely dotted, line width=1]
  (agent.west) --
    node[anchor=south,align=center] {}
  ([xshift=1.5cm]q-2.east);
\draw[->,dotted]
  (agent.west) --
    node[anchor=south,align=center] {}
  ([xshift=1.5cm]q-n.east);
\path ([xshift=0.5cm]q-2.south) -- ([xshift=0.5cm]q-n.north) node [font=\LARGE, midway, sloped] {$\dots$};
\end{tikzpicture}
\caption[Visualization of the queuing system when the agent activates queue 2]{Visualization of the queuing system when the agent activates queue 2, the bold lines represent the flow of jobs inside the system.}
\end{figure}

\hypertarget{evolution}{%
\subsubsection{Evolution}\label{evolution}}

In the continuos-time scale, each type of event (job arrival, job departure and environment changing) happens independently. That system is quite hard to program, therefore, we used a technique called ``uniformization'' to move from the continuos-time scale to discrete-time scale. In this time scale, at a given time, only one event can take place, regardless of their type.

In particular, given the system state is \((S, E)\), the agent decides to activate queue \(a\). One of these \(n + 1 + n(m - 1) + 1\) events can happen.

\begin{itemize}
\tightlist
\item
  The job of class \(Q_{i}\) increases by 1. Because there are \(n\) classes, we have \(n\) events of this kind with the rate \(\lambda_{1, M_{1}}, \dots, \lambda_{n, M_{n}}\) respectively (if the number of job of class \(Q_{i}\) reaches an upper limit, we consider \(\lambda_{i, M_{i}} = 0\)).
\item
  The job of class \(Q_{a}\) decreases by 1. Because we can only activate only one class, there is only one event of this type and its rate is \(\mu_{a, M_{a}}\).
\item
  The environment of class \(Q_{i}\) changes to a different environment other than \(m_{i}\) and the rate changing to environment \(j\) is \(\xi_{i, M_{i}, j}\). Because there are \(m - 1\) possible changes for each class and there are \(n\) classes, the number of this kind of event is \(n(m - 1)\).
\item
  And a special dummy event where nothing changes.
\end{itemize}

A discrete probability distribution is used to express that. In order to satisfy the condition of a probability distribution, all the rates above are divided by a normalization constant \(C\) to make sure that their sum are not greater than 1. If that sum is smaller than 1, the special dummy event is used to fill the gap so that the final sum will be equal to 1.

The normalization constant has the form as follow, which is deduced from the above evolution of the system.

\[
C = \sum_{i = 1}^{n} \max_{j} \lambda_{i, j} + \max_{i, j} \mu_{i, j} + \sum_{i = 1}^{n} \max_{j} \sum_{k = 1, k \neq j}^{m} \xi_{i, j, k} + \epsilon
\]

If \(\epsilon > 0\), the probability of the dummy event will always be greater than 0. In this internship, as we do not want the system to evolve too slowly, we choose \(\epsilon \coloneq 0\).

After obtaining all the information above, we use that discrete probability distribution to obtain the next transition \(T\) of the system and denote \(S'\) and \(E'\) the next state of the system.

\hypertarget{load-balancing-system}{%
\subsection{Load-balancing system}\label{load-balancing-system}}

\hypertarget{parameters-1}{%
\subsubsection{Parameters}\label{parameters-1}}

For the load-balancing system, we only have one stream of job arrivals with rate \(\lambda\) and \(n\) queues. Same as above, The queues capacities (holding cost and departure rate) also depend on which environment they are in. Therefore, we have similar parameters to the one of the queuing system. The only difference in this system is that instead of having multiple arrival rate that depends on the class of queue as well as the environment, we have only one global arrival rate \(\lambda\). A table summarizing the parameters of this system is shown below.

\begin{table}[!htbp]
\caption{Load-balancing system parameters}
\begin{center}
\begin{tabular}{c c c c c}
    \hline
    \multicolumn{2}{c}{} & $m_{1}$ & $\dots$ & $m_{m}$ \\
    \cline{2-5}
    \multirow{2}{*}{$Q_{1}$} &  Holding cost & $c_{1, 1}$ & $\dots$ & $c_{1, m}$ \\
    & Departure rate & $\mu_{1, 1}$ & $\dots$ & $\mu_{1, m}$ \\
    \cline{2-5}
    $\vdots$  \\
    \cline{2-5}
    \multirow{2}{*}{$Q_{n}$} &  Holding cost & $c_{n, 1}$ & $\dots$ & $c_{n, m}$ \\
    & Departure rate & $\mu_{n, 1}$ & $\dots$ & $\mu_{n, m}$ \\
    \cline{2-5}
    & Global arrival rate & & $\lambda$ & \\
    \hline
\end{tabular}
\end{center}
\label{tab:lbs-param}
\end{table}

\hypertarget{cost-1}{%
\subsubsection{Cost}\label{cost-1}}

Same as above, the cost of the system is a function of \(S\) and \(E\). However, in this system, only the simple linear version is used.

\[
f_{1}(S, E) = \sum_{1}^{n} c_{i, M_{i}} X_{i}
\]

\hypertarget{agent-1}{%
\subsubsection{Agent}\label{agent-1}}

The goal of the agent is the same: minimizing the cost of the whole system. He also receive the same information: the observable state \(S\). The difference here is instead of having to deactivating some queues, all queues now run continuously. His mission is to choose which queue to send the new arriving job to. Only the chosen queue will have new job arriving, the other queues only have to process the remaining jobs of them.

\begin{figure}
\centering
\begin{tikzpicture}[>=latex]
% the shapes
\path
  (0,3cm) pic {queuei=1}
  (0,1cm) pic {queuei=2}
  (0,-2cm) pic {queuei=n};
\path
  (1,4cm) coordinate (aux1)
  (1,-3.5cm) coordinate (aux2)
  (-0.5,0cm) coordinate (aux3)
  (2.5,0cm) coordinate (aux4);
\node[draw,dashed,text width=2.5cm,fit={(aux1) (aux2) (aux3) (aux4)}] (dashed) {};
\node[draw,align=center,circle,inner sep=2pt]
  at (-2,0) (agent)
  {Agent};

%the arrows
\draw[->, line width=1.25]
  ([xshift=-1cm]agent.west) --
    node[anchor=south,align=center] {$\lambda$}
  (agent.west);
\draw[->, dotted]
  (agent.east) --
    node[anchor=south,align=center] {}
  (q-1.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-1.east) --
    node[anchor=south,align=center] {$\mu_{1, M_{1}}$}
  ([xshift=4.5cm]q-1.east);
\draw[->, densely dotted, line width=1]
  (agent.east) --
    node[anchor=south,align=center] {}
  (q-2.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-2.east) --
    node[anchor=south,align=center] {$\mu_{2, M_{2}}$}
  ([xshift=4.5cm]q-2.east);
\draw[->, dotted]
  (agent.east) --
    node[anchor=south,align=center] {}
  (q-n.west);
\draw[->, line width=1.25]
  ([xshift=1.5cm]q-n.east) --
    node[anchor=south,align=center] {$\mu_{n, M_{n}}$}
  ([xshift=4.5cm]q-n.east);
\path ([xshift=0.5cm]q-2.south) -- ([xshift=0.5cm]q-n.north) node [font=\LARGE, midway, sloped] {$\dots$};
\end{tikzpicture}
\caption[Visualization of the load-balancing system when the agent sends job to queue 2]{Visualization of the load-balancing system when the agent sends job to queue 2, the bold lines represent the flow of jobs inside the system.}
\end{figure}

\hypertarget{evolution-1}{%
\subsubsection{Evolution}\label{evolution-1}}

In this system, given its state is \((S, E)\), the agent decides to send job to queue \(a\). One of these \(1 + n + n(m - 1) + 1\) events can happen.

\begin{itemize}
\tightlist
\item
  The job of class \(Q_{a}\) increases by 1. Because we can only send job to one queue, there is only one event of this type and its rate is \(\lambda\).
\item
  The job of class \(Q_{i}\) decreases by 1. Because there are \(n\) classes, we have \(n\) events of this kind with the rate \(\mu_{1, M_{1}}, \dots, \mu_{n, M_{n}}\) respectively (if the number of job of class \(Q_{i}\) reaches 0, we consider \(\mu_{i, M_{i}} = 0\)).
\item
  The environment of class \(Q_{i}\) changes to a different environment other than \(m_{i}\) and the rate changing to environment \(j\) is \(\xi_{i, M_{i}, j}\). Because there are \(m - 1\) possible changes for each class and there are \(n\) classes, the number of this kind of event is \(n(m - 1)\).
\item
  And a special dummy event where nothing changes.
\end{itemize}

The form of the normalization constant also changes according to the evolution of the system.

\[
C = \lambda + \sum_{i = 1}^{n} \max_{j} \mu_{i, j} + \sum_{i = 1}^{n} \max_{j} \sum_{k = 1, k \neq j}^{m} \xi_{i, j, k} + \epsilon
\]

\hypertarget{system-representation}{%
\subsection{System representation}\label{system-representation}}

In this internship, we limited ourselves on the cases where \(n = 2\) for both systems. A system of that kind can be represented as a grid where each side represents the evolution of each queue. In this report, we use the vertical side for the first queue, and the horizontal one for the second. Each cell of that grid represent a specific observable state of the system.

\begin{figure}
\centering
\def\mycolumns{5}
\def\myrows{5}
\begin{tikzpicture}[x=1cm, y=1cm, mynums/.style={inner sep=0}]
\fill[gray!50] (1,1) -- +(0, 1) -- +(1, 1) -- +(1,0) -- cycle;
\draw[step=1cm] (0,0) grid (\mycolumns + 1,\myrows + 1);
\foreach\x in {0,...,\mycolumns}
  \node[anchor=south, mynums] at (\x+0.5, \myrows + 1.25) {\x};
\foreach\x in {0,...,\myrows}
  \node[anchor=east, mynums] at (-0.25, \myrows - \x + 0.5) {\x};
\draw[->]
  (-1, \myrows + 0.5) --
    node[anchor=east,align=center] {$Q_{1}$}
  (-1, 0.5);
\draw[->]
  (0.5, \myrows + 2) --
    node[anchor=south,align=center] {$Q_{2}$}
  (\mycolumns + 0.5, \myrows + 2);
\end{tikzpicture}
\caption[Representation of a system with 2 queues]{Representation of a system with 2 queues and the length limit of both queues are 5, the gray cell represents an observable state of the system $S = (4, 1).$}
\end{figure}

\hypertarget{reinforcement-learning}{%
\section{Reinforcement learning}\label{reinforcement-learning}}

Reinforcement learning is used for training the agent to attain his goal. In this session, we present a short summary and introduce some related notions that could be useful for later.

\hypertarget{parameters-2}{%
\subsection{Parameters}\label{parameters-2}}

A simple reinforcement learning is modeled by a markov decision process (MDP) whose parameters include as follow:

\begin{itemize}
\tightlist
\item
  \(\mathcal{S}\): the set of all states of the system.
\item
  \(\mathcal{A}\): the set of all actions, and \(A_{S}\), the set of all actions available from state \(S\).
\item
  \(P(S_{t + 1} = S' | S_{t} = S, a_{t} = a)\): the probability that action \(a\) in state \(S\) at time \(t\) will lead to state \(S'\) at time \(t + 1\).
\item
  \(R(S' | S, a)\): the immediate reward received after transitioning from state \(S\) to state \(S'\), due to action \(a\).
\end{itemize}

In addition, we have policy \(\pi(a | S) = \mathbb{P}[A_{t} = a | S_{t} = S]\), a distribution over actions given states. In our problem, there is also an unobservable state inside our system, therefore, our problem is no longer an MDP, but instead a partial observable markov decision process (POMDP). We will now continue discussing about the MDP and return to the POMDP in later section.

\hypertarget{bellman-equation}{%
\subsection{Bellman equation}\label{bellman-equation}}

First, we want to try the solving method of a MDP.

The accumulate discounted future reward (return) from time \(t\) with the discount factor \(0 \le \gamma \le 1\) is:
\[
G_{t} = r_{t + 1} + \gamma r_{t + 2} + \gamma^{2} r_{t + 3} + \gamma^{3} r_{t + 4} + \dots = r_{t + 1} + \gamma G_{t + 1}
\]

From that, we have state-value function \(V_{\pi}(S)\) is the expected return starting from state \(S\), and then following policy \(\pi\):

\begin{equation}
V_{\pi}(S) = \mathbb{E}_{\pi}[G_{t} | S_{t} = S]
\label{eq:state-value}
\end{equation}

And action-value function \(Q_{\pi}(S, a)\) is the expected return starting from state \(S\), taking action \(a\), and then following policy \(\pi\):

\begin{equation}
Q_{\pi}(S, a) = \mathbb{E}_{\pi}[G_{t} | S_{t} = S, a_{t} = a]
\label{eq:action-value}
\end{equation}

From the fact that \(\pi(a | S)\) is a distribution over \(\mathcal{A}\) given \(S\), we have:

\begin{equation}
\begin{split}
V_{\pi}(S) {}&= \mathbb{E}_{\pi}[G_{t} | S_{t} = S] \\
  &= \sum_{a \in \mathcal{A}} \pi(a | S) \mathbb{E}_{\pi}[G_{t} | S_{t} = S, a_{t} = a] \\
  &= \sum_{a \in \mathcal{A}} \pi(a | S) Q_{\pi}(S, a)
\label{eq:v-to-q}
\end{split}
\end{equation}

Furthermore, we see that the expected future return of \(Q_{\pi}(S, a)\) is the sum of the current reward \(R(S)\) as well as the expected return of the next state regardless action multiply by the probability of moving to that state. Therefore, we have the equation below:

\begin{equation}
Q_{\pi}(S, a) = R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{\pi}(S')
\label{eq:q-to-v}
\end{equation}

Finally, the Bellman equations are obtained from \eqref{eq:v-to-q} and \eqref{eq:q-to-v}:

\begin{align}
\begin{split}
V_{\pi}(S) {}&= \sum_{a \in \mathcal{A}} \pi(a | S) Q_{\pi}(S, a) \\
  &= \sum_{a \in \mathcal{A}} \pi(a | S) (R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{\pi}(S')) \\
  &= R(S) + \gamma \sum_{a \in \mathcal{A}} \pi(a | S) \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{\pi}(S')
\label{eq:bellman-v-1}
\end{split} \\
\begin{split}
Q_{\pi}(S, a) {}&= R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{\pi}(S') \\
  &= R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) \sum_{a' \in \mathcal{A}} \pi(a' | S') Q_{\pi}(S', a')
\label{eq:bellman-q-1}
\end{split}
\end{align}

\hypertarget{optimal-policy-for-mdp}{%
\subsection{Optimal policy for MDP}\label{optimal-policy-for-mdp}}

The core idea of this problem is to find an optimal policy \(\pi_{*}\) that maximizes the expected reward or \(V(S)\). Mathematically, we have
\[
\pi > \pi' \text{ if } V_{\pi}(S) > V_{\pi'}(S) \,\, \forall S \in \mathcal{S}
\]

And we want to find
\[
\pi_{*} \text{ such that } \pi_{*} \ge \pi \,\, \forall \pi
\]

We can do that by finding the optimal state-value function \(V_{*}(S) = \max_{\pi} V_{\pi}(S)\) or action-value function \(Q_{*}(S, a) = \max_{\pi} Q_{\pi}(S, a)\) and define the optimal policy as follow:
\[
\pi_{*}(S) = \begin{cases}
1 & \text{ if } a = \arg\max_{a \in \mathcal{A}} Q_{*}(S, a) \\
0 & otherwise
\end{cases}
\]

Plugging all together into equation \eqref{eq:bellman-v-1} and \eqref{eq:bellman-q-1}, we have:

\begin{align}
V_{*}(S) &= R(S) + \max_{a \in \mathcal{A}} \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{*}(S')
\label{eq:bellman-max-v} \\
Q_{*}(S, a) &= R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) \max_{a' \in \mathcal{A}} Q_{*}(S', a')
\label{eq:bellman-max-q}
\end{align}

\hypertarget{efficient-policy-when-having-unobservable-environment}{%
\subsection{Efficient policy when having unobservable environment}\label{efficient-policy-when-having-unobservable-environment}}

Our problem is a POMDP, the parameters could be seen as follow:

\begin{itemize}
\tightlist
\item
  \(\mathcal{S}\): the set of observable states is represented as the grid.
\item
  \(\mathcal{E}\): the set of unobservable environment states.
\item
  \(\mathcal{A}\): the action of activating / sending work to a queue.
\item
  \(P(S', E' | S, E, a)\): the transition probability that is clarified in the evolution section and depends on the both visible and hidden state (S, E).
\item
  \(R(S' | S, E, a)\): the immediate reward is \(-C\) where \(C\) is the total holding cost of the system. Since this function depends only on the the state (S, E) of system, we could write \(R(S, E)\) instead.
\end{itemize}

Since the goal of the internship is not about solving the POMDP, in the later sections, we will explore how learning algorithms of MDP work in the context of unobservable environment.

\hypertarget{online-q-learning}{%
\section{Online Q-learning}\label{online-q-learning}}

\hypertarget{about-the-algorithm}{%
\subsection{About the algorithm}\label{about-the-algorithm}}

Online Q-learning is probably one of the most simple and popular algorithm for reinforcement learning problem. It is used to estimate \(Q_{*}(S, a) \,\, \forall S \in \mathcal{S} \text{ and } a \in \mathcal{A}\) while interacting with the system. This algorithm only requires the real system or a simulator to interact with, and not the internal settings of that system, which makes it fit the context of this internship and more broadly real-life for a MDP, where these settings can change at any time. Beside its simplicity and close to real life context, the algorithm is also proven to converge almost surely when the number of visits to each state goes to \(\infty\) in \autocite{q-learning-converge}. Visually, we can imagine that our agent will follow a grid trajectory of states and try to discover what is the action-value at that state with a specific action.

\begin{figure}
\centering
\def\mycolumns{5}
\def\myrows{5}
\begin{tikzpicture}[x=1cm, y=1cm, mynums/.style={inner sep=0}]
\fill[gray!50] (0,5) -- +(0, 1) -- +(1, 1) -- +(1,0) -- cycle;
\draw[step=1cm] (0,0) grid (\mycolumns + 1,\myrows + 1);
\foreach\x in {0,...,\mycolumns}
  \node[anchor=south, mynums] at (\x+0.5, \myrows + 1.25) {\x};
\foreach\x in {0,...,\myrows}
  \node[anchor=east, mynums] at (-0.25, \myrows - \x + 0.5) {\x};
\draw[->]
  (-1, \myrows + 0.5) --
    node[anchor=east,align=center] {$Q_{1}$}
  (-1, 0.5);
\draw[->]
  (0.5, \myrows + 2) --
    node[anchor=south,align=center] {$Q_{2}$}
  (\mycolumns + 0.5, \myrows + 2);
\draw[->, line width=1.25] (0.5, 5.5) -- (1.5, 5.5) -- (1.5, 3.5) -- (0.5, 3.5) -- (0.5, 2.5) -- (3.5, 2.5);
\end{tikzpicture}
\caption[An example of the path which the agent might take]{An example of the path which the agent might take. It starts from $(0, 0)$ and moves to $(3, 3)$. The next state could be either $(3, 4), (4, 3), (3, 2), (2, 3)$ depends on the action it chooses and the transition probabilities.}
\end{figure}

In this report, we want to investigate the Q-learning algorithm when applied to our POMDP. We used the following algorithm:

\begin{algorithm}
\caption{Online Q-Learning}\label{alg:on-q-learning}
\KwIn{$T > 0$ number iterations, $\gamma$ discount factor, $\epsilon$ exploration factor, $\eta$ learning rate}
\KwOut{$Q_{*}$, $\pi_{*}$}
\For {$t \gets 0$ \KwTo $T$} {
  $S, E \gets \mathfrak{S}$ current state of the system\;
  $a \gets 
    \begin{cases}
    \begin{aligned}
      \text{one random possible action}&, && \text{ with the probability of } \epsilon \text{ (exploration)}\\ 
      \arg\max_{a \in \mathcal{A}} Q_{t}(S, a)&, && \text{ with the probability of } 1 - \epsilon \text{ (learning)}
    \end{aligned}
    \end{cases}
  $\\
  \tcc{$S' = S$ if $E' \neq E$ and vice versa}
  $R, S', E' \gets \mathfrak{S}(a, S, E)$\;
  $Q_{t+1}(S, a) \gets Q_{t}(S, a) + \eta (R + \gamma \max_{a' \in \mathcal{A}} Q_{t}(S', a') - Q_{t}(S, a))$ \;
}
\end{algorithm}

\hypertarget{implementation}{%
\subsection{Implementation}\label{implementation}}

\hypertarget{from-python-to-c}{%
\subsubsection{From Python to C++}\label{from-python-to-c}}

In this section, we document some difficulties while implementing the framework and how we overcome it. For illustration purpose, we will test the algorithm with a very simple problem of the \protect\hyperlink{queuing-system}{queuing system}
and linear cost function with no environment (or constant environment). The parameter of this system is show in table \ref{tab:q-learning-test}.

\begin{table}[!htbp]
\caption{Simple queuing problem for testing}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $2$ \\
    & Arrival rate $\lambda_{1}$ & $0.135$ \\
    & Departure rate $\mu_{1}$ & $0.3$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $3$ \\
    & Arrival rate $\lambda_{2}$ & $0.135$ \\
    & Departure rate $\mu_{2}$ & $0.3$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:q-learning-test}
\end{table}

It has been proven that if we have \(c_{1} \mu_{1} \ge c_{2} \mu_{2}\), the optimal policy for all cases is activating the first queue if possible, and vice versa in the case where \(c_{1} \mu_{1} \le c_{2} \mu_{2}\). From table \ref{tab:q-learning-test}, we know that our optimal action here is activating the second queue.

With a naive Python implementation, the algorithm works correctly when the limit of both queue are small.

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{report_files/figure-latex/3x3-python-1} 

}

\caption{Result of online Q-learning using Python for 3x3 case. $\gamma = 0.999$, $\epsilon = 0.99$}\label{fig:3x3-python}
\end{figure}

First, we see that on the first column where \(Q_{2} = 0\), our agent always chooses to activate the first queue, because this is the only option possible, because there is no job on the second queue. Same thing holds for the first row where \(Q_{1} = 0\). For the other cells, we have the black color which means the agent chooses the second queue which is aligned with the analytical solution as above.

This implementation runs \(10^6\) iterations for approximately \(2\) minutes. This is quite good for number with small case. However, if we increase the limit to 10, with the same number of iterations gives a imperfect result.

\begin{figure}

{\centering \subfloat[For $10^6$ iterations\label{fig:10x10-python-1}]{\includegraphics[width=0.4\linewidth]{report_files/figure-latex/10x10-python-1} }\subfloat[For $10^7$ iterations\label{fig:10x10-python-2}]{\includegraphics[width=0.4\linewidth]{report_files/figure-latex/10x10-python-2} }

}

\caption{Result of online Q-learning using Python for 10x10 case. $\gamma = 0.99$, $\epsilon = 0.999$}\label{fig:10x10-python}
\end{figure}

In the figure above, we can see that even after 20 minutes with \(10^7\) iterations, the algorithm still does not converge completely yet. To have a clear view about the reason ưhy, we have figure \ref{fig:10x10-python-n-visit}.

\begin{figure}

{\centering \subfloat[Activating $Q_{1}$\label{fig:10x10-python-n-visit-1}]{\includegraphics[width=0.4\linewidth]{report_files/figure-latex/10x10-python-n-visit-1} }\subfloat[Activating $Q_{2}$\label{fig:10x10-python-n-visit-2}]{\includegraphics[width=0.4\linewidth]{report_files/figure-latex/10x10-python-n-visit-2} }

}

\caption{Number of visit for the system above. $\gamma = 0.99$, $\epsilon = 0.999$}\label{fig:10x10-python-n-visit}
\end{figure}

We can see that our agent is stuck at \((0, 0)\), only the area around the origin has a high number of visits. The rest have relatively low number of visits and therefore, does not converge because of the nonfulfillment of the condition in \autocite{q-learning-converge}.

To speed up the algorithm, we decided to switch to C++, which is famous for its speed and its mature support for high-speed computation with library like \passthrough{\lstinline!Eigen!} and then expose a Python interface for more ease of use. This is also the same approach for many scientific libraries in Python such as \passthrough{\lstinline!Numpy!} or \passthrough{\lstinline!Tensorflow!}. After switching to C++ and some improvements later, the algorithm now can run \(10^9\) iterations in only 1 minute, a \(2 \times 10^3\) speed up.

\hypertarget{from-n-dimensions-to-2-dimensions}{%
\subsubsection{\texorpdfstring{From \(n\)-dimensions to \(2\)-dimensions}{From n-dimensions to 2-dimensions}}\label{from-n-dimensions-to-2-dimensions}}

With C++, the simulator is already running faster. However, there are still room for improvements. Now, the probability \(P(S', E' | S, E, a)\) is calculated on-the-fly when we meet that state \((S, E)\) while running the algorithm, we could make it faster by calculate the matrix transition for all the states beforehand. However, the state \(S\) and \(E\) of the system are each represented by a vector of \(n\) entries, if we keep using vector like that, we will need a tensor of \(n + n + 1 = 2N + 1\) dimensions (first \(n\) dimensions for saving the observable state \(S\), the next \(n\) dimensions for saving the unobservable state \(S\) and the last one for action). In addition, since there is no sparse version for high-dimensional tensor, it will take a lot of memory to save that tensor, more precisely:

\[
(L_{1} + 1) \times \dots \times (L_{n} + 1) \times \underbrace{n^{m}}_{n \text{ queues and } m \text{ environments}} \times \underbrace{n}_{n \text{ actions}}
\]

If we try to allocate a tensor of that size, we will get an ``out of memory'' error.

Furthermore, in other algorithms, we are required to iterate all over the set of states. If \(n\), the number of queues is static, we could do something similar to algorithm \ref{alg:states-3}. However our goal is to let \(n\) as a input for the program. So we can not use that algorithm there.

\begin{algorithm}
\caption{Iteration over the set of visible states $\mathcal{S}$ when $n = 3$} \label{alg:states-3}
\For {$i \gets 0$ \KwTo $L_{1}$} {
  \For {$j \gets 0$ \KwTo $L_{2}$} {
    \For {$k \gets 0$ \KwTo $L_{3}$} {
      $S \gets (i, j, k)$\;
      \dots\;
    }
  }
}
\end{algorithm}

In the end, we decide to encode the whole state into an integer by extending the notion of base of number. Given state \((S, E)\), we introduce 3 functions:
\[
\begin{aligned}
f_{\mathcal{S}}(S) &= f(X_{1}, \dots, X_{n}) = X_{1} L_{2} \dots L_{n} + X_{2} L_{3} \dots L_{n} + \dots + X_{n - 1} L_{n} + X_{n} \\
f_{\mathcal{E}}(E) &= f(M_{1}, \dots, M_{n}) = M_{1} m^{n - 1} + M_{2} m^{n - 2} + \dots + M_{n - 1} m + M_{n} \\
f_{\mathcal{S, E}(S, E)} &= f(X_{1}, \dots, X_{n}, M_{1}, \dots, M_{n}) = m^{n} f_{\mathcal{S}}(S) + f_{\mathcal{E}}(E)
\end{aligned}
\]

All three functions are one-to-one mapping, so we can replace the state with the result of these function without losing anything. Because of doing so, we have achieved several benefits:

\begin{itemize}
\tightlist
\item
  We can calculate the transition matrix right now and use a sparse matrix to store it for efficiency in both storage and performance (since the transition matrix will be full of 0).
\item
  Turn a problem with unknown dimension to a 2-dimensional one which makes accessing and looping through the states easier.
\item
  Overall speed improvement since we only need to deal with integer instead of a vector.
\end{itemize}

\hypertarget{offline-q-learning}{%
\section{Offline Q-learning}\label{offline-q-learning}}

\hypertarget{about-the-algorithm-1}{%
\subsection{About the algorithm}\label{about-the-algorithm-1}}

In this version of Q-learning, we do not explore or follow the trajectory as in the online version, but we will force the calculation of the Q-value for all pairs \((S, a)\). This method is faster than online Q-learning, however, it requires a reliable simulator of the system to be able to work. The algorithm is described below:

\begin{algorithm}
\caption{Offline Q-Learning}\label{alg:off-q-learning}
\KwIn{$T > 0$ number iterations, $\gamma$ discount factor, $\eta$ learning rate}
\KwOut{$Q_{*}$, $\pi_{*}$}
$\mathfrak{S}$ simulator of the system\;
$E$ initial environment state of the system\;
\For {$t \gets 0$ \KwTo $T$} {
  $ -, -, E' \gets \mathfrak{S}(-, -, E)$\;
  \If{$E \ne E'$}{
    $E \gets E'$ \;
  }
  \For {$S \in \mathcal{S}$} {
    \For {$a \in \mathcal{A}$} {
      $S', R, - \gets \mathfrak{S}(a, S, E)$\;
      $Q_{t+1}(S, a) \gets Q_{t}(S, a) + \eta (R + \gamma \max_{a'} Q_{t}(S', a') - Q_{t}(S, a))$\;
    }
  }
}
\end{algorithm}

\hypertarget{value-iteration}{%
\section{Value iteration}\label{value-iteration}}

\hypertarget{about-the-algorithm-2}{%
\subsection{About the algorithm}\label{about-the-algorithm-2}}

This algorithm is directly based on equation \eqref{eq:bellman-max-v} and used to estimate the value of \(V_{*}(S)\). After obtaining an estimate of \(V_{*}(S)\), the policy is deduced as \(\pi_{*}(S) = \arg\max_{a} R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{*}(S')\). The algorithm is written as follow:

\begin{algorithm}
\caption{Value iteration} \label{alg:value-iteration}
\KwIn{$\gamma$ discount factor}
\KwOut{$V_{*}$, $\pi_{*}$}
$\Delta \gets \infty$\;
\While {\Delta > \epsilon} {
  \For {$S \in \mathcal{S}$} {
    $V_{t+1}(S) \gets \max_{a \in \mathcal{A}} R(S) + \gamma \sum_{S' \in \mathcal{S}} P(S' | S, a) V_{*}(S')$\;
  }
  $\Delta \gets \lVert V_{t + 1} - V_{t} \rVert$\_{2};
}
\end{algorithm}

We can see above that the algorithm makes use of \(P(S' | S, a)\), because of this, this algorithm converge a lot faster than both Q-learning methods but we have to know the transition probability, so this method is use only as a baseline to compare with the result of Q-learning.

If we have only one constant environment \(E_{c}\) (or no environment changing), we could rewrite \(P(S', E_{c} | S, E_{c}, a) = P(S' | S, a)\) and value iterations work correctly. However, as we have unobservable environment, algorithm \ref{alg:value-iteration} can not work without modification because we have \(P(S', E' | S, E, a)\) but we want to estimate \(V_{*}(S)\) and not \(V_{*}(S, E)\). In the next section, we discuss about how to get an empirical estimate of \(P(S'|S, a)\) based on \(P(S', E' | S, E, a)\).

\hypertarget{empirical-transition-probability}{%
\subsection{Empirical transition probability}\label{empirical-transition-probability}}

Our idea is to calculate the probability of the system stay in a particular state \(E \in \mathcal{E}\) and then multiply it with the transition probability to get the average probability. First, we have some equations:

\begin{equation}
  P(S'|S, E, a) = 
    \begin{cases}
        P(S', E|S, E, a) \quad & \text{if} \, S' \neq S \\
        1 - \sum_{S' \neq S} P(S', E|S, E, a) = \sum_{E'} P(S, E'|S, E, a) \quad & \text{otherwise} \\
    \end{cases}
\label{eq:p-s}
\end{equation}

\begin{equation}
  P(E'|S, E, a) = 
    \begin{cases}
        P(S, E'|S, E, a) \quad & \text{if} \, E' \neq E \\
        1 - \sum_{E' \neq E} P(S, E'|S, E, a) \quad & \text{otherwise} \\
    \end{cases}
\label{eq:p-e}
\end{equation}

The logic behind equation \eqref{eq:p-s} and \eqref{eq:p-e} is the fact that only one event can happen at a time.

\begin{equation}
  P(S, E'|S, E, a) = 
    \begin{cases}
        \xi_{i, M_{i}, M_{i}'} \quad & \text{if} \, E' \text{ and } E \text{ differ by only one entry at index } i \\
        0 \quad & \text{else} \\
    \end{cases}
\label{eq:p-e-e}
\end{equation}

By rewriting \(P(S, E' | S, E, a)\) as in equation \eqref{eq:p-e-e}, we see that \(P(E'|S, E, a)\) does not depend on \(S, a\) and therefore \(P(E'|S, E, a) = P(E'| E)\). From that, we could calculate the steady state by an iterative method using the equation below:

\begin{equation}
\label{eq:steady-prob}
    \pi_{t + 1}(E') = \sum_{E \in \mathcal{E}} P(E'| E) \pi_{t}(E)
\end{equation}

After \(\pi(E)\) converges to \(\tilde{\pi}(E)\) and using \eqref{eq:p-s}, we have the final empirical value we want:

\begin{equation}
\label{eq:p-tilde}
    \tilde{P}(S'| S, a) = \sum_{E \in \mathcal{E}} \tilde{\pi}(E) P(S'| S, E, a)
\end{equation}

\hypertarget{result}{%
\section{Result}\label{result}}

\hypertarget{no-environment-changing-mdp}{%
\subsection{No environment changing (MDP)}\label{no-environment-changing-mdp}}

\hypertarget{queuing}{%
\subsubsection{Queuing}\label{queuing}}

First we will try with the linear cost. This is the simplest case and since it is proven mathematically (?), it is only a test to show that all algorithms works correctly. We will try two tuples of parameters, the first one will be:

\begin{table}[!htbp]
\caption{Simple queuing 1}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $3$ \\
    & Arrival rate $\lambda_{1}$ & $0.13$ \\
    & Departure rate $\mu_{1}$ & $0.3$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $1$ \\
    & Arrival rate $\lambda_{2}$ & $0.13$ \\
    & Departure rate $\mu_{2}$ & $0.3$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:simple-queuing-1}
\end{table}

The theoretical optimal action here is \(1\).



\begin{figure}

{\centering \subfloat[Online Q-learning\label{fig:20x20-simple-queuing-1-1}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-queuing-1-1} }\subfloat[Offline Q-learning\label{fig:20x20-simple-queuing-1-2}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-queuing-1-2} }\subfloat[Value iterations\label{fig:20x20-simple-queuing-1-3}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-queuing-1-3} }

}

\caption{Result of table \ref{tab:simple-queuing-1} for 20x20 case. \(\gamma = 0.999\), \(\epsilon = 0.999\).}\label{fig:20x20-simple-queuing-1}
\end{figure}

And the second one is nearly the same, except the holding cost is reverse, and therefore the optimal action here is \(2\):

\begin{table}[!htbp]
\caption{Simple queuing 2}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $1$ \\
    & Arrival rate $\lambda_{1}$ & $0.13$ \\
    & Departure rate $\mu_{1}$ & $0.3$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $3$ \\
    & Arrival rate $\lambda_{2}$ & $0.13$ \\
    & Departure rate $\mu_{2}$ & $0.3$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:simple-queuing-2}
\end{table}



\begin{figure}

{\centering \subfloat[Online Q-learning\label{fig:20x20-simple-queuing-2-1}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-queuing-2-1} }\subfloat[Offline Q-learning\label{fig:20x20-simple-queuing-2-2}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-queuing-2-2} }\subfloat[Value iterations\label{fig:20x20-simple-queuing-2-3}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-queuing-2-3} }

}

\caption{Result of table \ref{tab:simple-queuing-2} for 20x20 case. \(\gamma = 0.999\), \(\epsilon = 0.999\).}\label{fig:20x20-simple-queuing-2}
\end{figure}

As we can see, all three algorithms work well for this simple case. We will move to more complex case.

\hypertarget{load-balancing}{%
\subsubsection{Load-balancing}\label{load-balancing}}

For this system, We first try a system where two queues have the same parameters:

\begin{table}[!htbp]
\caption{Simple load-balancing 1}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $3$ \\
    & Departure rate $\mu_{1}$ & $0.3$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $3$ \\
    & Departure rate $\mu_{2}$ & $0.3$ \\
    \cline{2-3}
    & Global arrival rate & $0.5$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:simple-lb-1}
\end{table}



\begin{figure}

{\centering \subfloat[Online Q-learning\label{fig:20x20-simple-lb-1-1}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-lb-1-1} }\subfloat[Offline Q-learning\label{fig:20x20-simple-lb-1-2}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-lb-1-2} }\subfloat[Value iterations\label{fig:20x20-simple-lb-1-3}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-lb-1-3} }

}

\caption{Result of table \ref{tab:simple-lb-1} for 20x20 case. \(\gamma = 0.999\), \(\epsilon = 0.999\).}\label{fig:20x20-simple-lb-1}
\end{figure}

Intuitively, if two queues has the same parameters, the optimal action will be sending job to the queue that has less jobs, which is the result of value iterations method (the right grid) of figure \ref{fig:20x20-simple-lb-1}. There is also a gray color indicates that at those states, either action \(1\) (sending new job to \(Q_{1}\)) or \(2\) (sending to new job to \(Q_{2}\)) leads to the same result, because they have the same number of jobs.

For two Q-learning methods, the online and offline version has been trained for \(3 \times 10^{10}\) and \(5 \times 10^{7}\) iterations respectively and takes 30 minutes each. They give us a similar result and both are close to the optimal policy of value iterations.

However, since both methods depends on the evolution of the system, they are very sensitive to the parameters of systems. For example, if we increase the arrival rate to \(0.55\):

\begin{table}[!htbp]
\caption{Simple load-balancing 2}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $3$ \\
    & Departure rate $\mu_{1}$ & $0.3$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $3$ \\
    & Departure rate $\mu_{2}$ & $0.3$ \\
    \cline{2-3}
    & Global arrival rate & $0.55$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:simple-lb-2}
\end{table}

Although the optimal policy should be the same as above, the same number of iterations give us a different result.



\begin{figure}

{\centering \subfloat[Online Q-learning\label{fig:20x20-simple-lb-2-1}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-lb-2-1} }\subfloat[Offline Q-learning\label{fig:20x20-simple-lb-2-2}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-lb-2-2} }\subfloat[Value iterations\label{fig:20x20-simple-lb-2-3}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-lb-2-3} }

}

\caption{Result of table \ref{tab:simple-lb-2} for 20x20 case. \(\gamma = 0.999\), \(\epsilon = 0.999\).}\label{fig:20x20-simple-lb-2}
\end{figure}

The offline version is similar to the policy of value iterations method and has a smaller hole compared to the online version. The difference can be explained by the fact that in the offline version, each pair state-action is visited \(5 \times 10^{7}\) times, so the total number of iterations will be \(2 \times 41^{2} \times 5 \times 10^{7} \approx 1.6 \times 10^{11}\), which is \(\gg 3 \times 10^{10}\), the total number of iterations of the online version.

Furthermore, even if we run the online version with that much iterations, it will still not guarantee that we will have a good approximation of the offline result. This is because we are following the trajectory (or the evolution of the system) and therefore it is impossible to visit all states equally. We can see the number of visits of each pair state-action below.



\begin{figure}

{\centering \subfloat[Sending to $Q_{1}$\label{fig:20x20-simple-lb-2-n-visit-1}]{\includegraphics[width=0.4\linewidth]{report_files/figure-latex/20x20-simple-lb-2-n-visit-1} }\subfloat[Sending to $Q_{2}$\label{fig:20x20-simple-lb-2-n-visit-2}]{\includegraphics[width=0.4\linewidth]{report_files/figure-latex/20x20-simple-lb-2-n-visit-2} }

}

\caption{Number of visits of online Q-learning on table \ref{tab:simple-lb-2} for 20x20 case.}\label{fig:20x20-simple-lb-2-n-visit}
\end{figure}

The number of visits is high around \((0, 0)\) and \((40, 40)\). Central region of the grid receives much less visits. Therefore, we see in figure \ref{fig:20x20-simple-lb-1} that the diagonal is beginning to form from these two points but not the region near the grid center.

We will the finish this section with an unbalance example. We fix the cost and make the departure rate of the first queue higher than the second one as follow:

\begin{table}[!htbp]
\caption{Simple load-balancing 3}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $3$ \\
    & Departure rate $\mu_{1}$ & $0.4$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $3$ \\
    & Departure rate $\mu_{2}$ & $0.3$ \\
    \cline{2-3}
    & Global arrival rate & $0.6$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:simple-lb-3}
\end{table}



\begin{figure}

{\centering \subfloat[Online Q-learning\label{fig:20x20-simple-lb-3-1}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-lb-3-1} }\subfloat[Offline Q-learning\label{fig:20x20-simple-lb-3-2}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-lb-3-2} }\subfloat[Value iterations\label{fig:20x20-simple-lb-3-3}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-simple-lb-3-3} }

}

\caption{Result of table \ref{tab:simple-lb-3} for 20x20 case. \(\gamma = 0.999\), \(\epsilon = 0.999\).}\label{fig:20x20-simple-lb-3}
\end{figure}

Once again, we see the sensitivity of Q-learning with the system settings. Intuitively, since class \(1\) works faster than class \(2\) with the same cost, we will send more jobs to the first one as in the value iterations method. However, the learning capacity of both Q-learning methods is not sufficient to capture all the information of the system.

We then now move to the cases of multiple environments.

\hypertarget{multiple-environments-pomdp}{%
\subsection{Multiple environments (POMDP)}\label{multiple-environments-pomdp}}

We will introduce some new notations of environment transitions. We note that after uniformization, the probability of transitioning from one environment to itself (the diagonal of table \ref{tab:mat-transition-ci}) does not matter any more since it will be taken account into the dummy event. Futhermore, the sum of the other probabilities does not need to be less than \(1\) as we will divide them with the normalization constant.

The more important thing now is the ratio between each probabilities pair. We can rewrite table \ref{tab:mat-transition-ci} as follow:

\begin{table}[!htbp]
\caption{Uniformized matrix transition of the environment of $Q_{i}$}
\begin{center}
\begin{tabular}{c c c c c}
    \hline
    & $m_{1}$ & $m_{2}$ & $\dots$ & $m_{m}$ \\
    $m_{1}$ & $*$ & $\xi_{i, 1, 2}$ & \dots & $\xi_{i, 1, m}$ \\
    $m_{2}$ & $\xi_{i, 2, 1}$ & $*$ & \dots & $\xi_{i, 2, m}$ \\
    $\vdots$ & & & & \\
    $m_{m}$ & $\xi_{i, m, 1}$ & $\xi_{i, m, 2}$ & \dots & $*$ \\
    \hline
\end{tabular}
\end{center}
\label{tab:uni-mat-transition-ci}
\end{table}

Furthermore, because we want to see if the speed of environment changing affects the optimal policy of each method, we multiply all transition matrices of each \(Q_{i}\) of the system with \(\alpha\). This does not change the ratio mentioned above and only takes effect when uniformization. In this process, it acts as a ratio between the change in the observable state and non observable (environment) state. If \(\alpha\) is small, the rate of environment changing will only take a small part in the uniformized probability and vice versa.

From all the small transition matrices of each \(Q_{i}\), we can get a big environment transition matrix \(\mathcal{M}_{\mathcal{E}}\) where each row and column represents an environment state encoded using the method present in \protect\hyperlink{from-n-dimensions-to-2-dimensions}{From \(n\)-dimensions to \(2\)-dimensions} section. Since all the entries except the ones on the diagonal are multiplied by \(\alpha\), we can express \(\mathcal{M}_{\mathcal{E}}\) as follow:

\[
\mathcal{M}_{\mathcal{E}} = \begin{pmatrix}
\rho_{1, 1} & \alpha \rho_{1, 2} & \dots & \alpha \rho_{1, |\mathcal{E}|} \\
\alpha \rho_{2, 1} & \rho_{2, 2} & \dots & \alpha \rho_{2, |\mathcal{E}|} \\
\vdots & & & \vdots \\
\alpha \rho_{|\mathcal{E}|, 1} & \alpha \rho_{2, 2} & \dots & \rho_{|\mathcal{E}|, |\mathcal{E}|} \\
\end{pmatrix}
\]

Where for example if \((m_{1}, m_{1}, \dots, m_{1}, m_{1}) \mapsto i\) and \((m_{1}, m_{1}, \dots, m_{1}, m_{2}) \mapsto j\), then \(\rho_{i, j}\) will be equal to \(\xi_{n, 1, 2}\) (the probability that the environment of queue \(n\) moves from \(m_{1}\) to \(m_{2}\)).

If \(\Pi_{\mathcal{E}}\) is the solution of equation \eqref{eq:steady-prob} then \(\mathcal{M}_{\mathcal{E}} \Pi_{\mathcal{E}} = \Pi_{\mathcal{E}}\) (because \(P(E' | E)\) is actually \(\rho\)).

Lastly, with all the notion above, we can prove that the solution of
equation \eqref{eq:steady-prob} does not depend on \(\alpha\).

\begin{lemma*}
Given a set of all $\rho_{i, j} \, (i \neq j)$ defined as above.

We call a pair of $(\alpha, (\rho_{1, 1}, \dots, \rho_{|\mathcal{E}|, |\mathcal{E}|}))$ satisfied if $\mathcal{M}_{\alpha}$ is a right stochastic matrix where:

$$
\mathcal{M}_{\alpha} = \begin{pmatrix}
\rho_{1, 1} & \alpha \rho_{1, 2} & \dots & \alpha \rho_{1, |\mathcal{E}|} \\
\alpha \rho_{2, 1} & \rho_{2, 2} & \dots & \alpha \rho_{2, |\mathcal{E}|} \\
\vdots & & & \vdots \\
\alpha \rho_{|\mathcal{E}|, 1} & \alpha \rho_{2, 2} & \dots & \rho_{|\mathcal{E}|, |\mathcal{E}|} \\
\end{pmatrix}
$$

If $(\alpha, (\rho_{1, 1}, \dots, \rho_{|\mathcal{E}|, |\mathcal{E}|}))$ and $(\alpha', (\rho_{1, 1}', \dots, \rho_{|\mathcal{E}|, |\mathcal{E}|}'))$ are satisfied, and there is $\Tau$ such that $\mathcal{M}_{\alpha} \Tau = \Tau$, then $\mathcal{M}_{\alpha'} \Tau = \Tau$
\end{lemma*}

\begin{proof}
From $\mathcal{M}_{\alpha} \Tau = \Tau$, we have:

\begin{align*}
\iff & \alpha \sum_{j, j \neq i} \rho_{i, j} \tau_{j} + \rho_{i, i} \tau_{i} = \tau_{i} \\
\iff & \alpha \sum_{j, j \neq i} \rho_{i, j} \tau_{j} + (1 - \alpha \sum_{j, j \neq i} \rho_{i, j}) \tau_{i} = \tau_{i} \\
\iff & \alpha \sum_{j, j \neq i} \rho_{i, j} \tau_{j} - \alpha \tau_{i} \sum_{j, j \neq i} \rho_{i, j} = 0 \\
\iff & \sum_{j, j \neq i} \rho_{i, j} \tau_{j} = \tau_{i} \sum_{j, j \neq i} \rho_{i, j} \\
\iff & \alpha' \sum_{j, j \neq i} \rho_{i, j} \tau_{j} = \alpha' \tau_{i} \sum_{j, j \neq i} \rho_{i, j} \\
\iff & \alpha' \sum_{j, j \neq i} \rho_{i, j} \tau_{j} + (1 - \alpha' \sum_{j, j \neq i} \rho_{i, j}) \tau_{i} = \tau_{i} \\
\iff & \alpha' \sum_{j, j \neq i} \rho_{i, j} \tau_{j} + \rho_{i, i}' \tau_{i} = \tau_{i} \\
\end{align*}

Therefore, $\mathcal{M}_{\alpha'} \Tau = \Tau$.
\end{proof}

\hypertarget{queuing-1}{%
\subsubsection{Queuing}\label{queuing-1}}

\hypertarget{small-alpha}{%
\paragraph{\texorpdfstring{Small \(\alpha\)}{Small \textbackslash alpha}}\label{small-alpha}}

We start with a simple parameter as below:

\begin{table}[!htbp]
\caption{Queuing 1}
\begin{center}
\begin{tabular}{c c c c}
    \hline
    \multicolumn{2}{c}{} & $m_{1}$ & $m_{2}$ \\
    \cline{2-4}
    \multirow{3}{*}{$C_{1}$} & Holding cost & $5$ & $1$ \\
    & Arrival rate & $0.13$ & $0.13$ \\
    & Departure rate & $0.3$ & $0.3$ \\
    \cline{2-4}
    \multirow{3}{*}{$C_{2}$} & Holding cost & $2$ & \\
    & Arrival rate & $0.13$ & \\
    & Departure rate & $0.3$ & \\    
    \hline
\end{tabular}
\label{tab:queuing-1}
\end{center}
\end{table}

\begin{table}[!htbp]
\caption{Queuing 2}
\begin{center}
\begin{tabular}{c c c c}
    \hline
    \multicolumn{2}{c}{} & $m_{1}$ & $m_{2}$ \\
    \cline{2-4}
    \multirow{3}{*}{$C_{1}$} & Holding cost & $1$ & $5$ \\
    & Arrival rate & $0.13$ & $0.13$ \\
    & Departure rate & $0.3$ & $0.3$ \\
    \cline{2-4}
    \multirow{3}{*}{$C_{2}$} & Holding cost & $2$ & \\
    & Arrival rate & $0.13$ & \\
    & Departure rate & $0.3$ & \\    
    \hline
\end{tabular}
\label{tab:queuing-2}
\end{center}
\end{table}

\begin{table}[!htbp]
\caption{Matrix transition of the environment of $Q_{1}$ of table \ref{tab:queuing-1} and table \ref{tab:queuing-2}}
\begin{center}
\begin{tabular}{c c c}
    \hline
    & $m_{1}$ & $m_{2}$ \\
    $m_{1}$ & $*$ & $1$ \\
    $m_{2}$ & $1$ & $*$ \\
    \hline
    $\alpha$ & \multicolumn{2}{c}{$10^{-3}$} \\
    \hline
\end{tabular}
\end{center}
\label{tab:queuing-1-mat-1}
\end{table}



\begin{figure}

{\centering \subfloat[Online Q-learning\label{fig:20x20-queuing-1-2-1}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-queuing-1-2-1} }\subfloat[Offline Q-learning\label{fig:20x20-queuing-1-2-2}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-queuing-1-2-2} }\subfloat[Value iterations\label{fig:20x20-queuing-1-2-3}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-queuing-1-2-3} }\newline\subfloat[Online Q-learning\label{fig:20x20-queuing-1-2-4}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-queuing-1-2-4} }\subfloat[Offline Q-learning\label{fig:20x20-queuing-1-2-5}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-queuing-1-2-5} }\subfloat[Value iterations\label{fig:20x20-queuing-1-2-6}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-queuing-1-2-6} }

}

\caption{Result of table \ref{tab:queuing-1} (upper) and \ref{tab:queuing-2} (lower) for 20x20 case. \(\gamma = 0.999\), \(\epsilon = 0.999\).}\label{fig:20x20-queuing-1-2}
\end{figure}

First, for both cases, the solution of equation \eqref{eq:steady-prob} is \(\Pi = (0.5, 0.5)\) with \(\mathcal{E} = \{(m_{1}, m_{1}), (m_{2}, m_{1})\}\) (since queue \(2\) has only one environment). Furthermore, from equation \eqref{eq:p-tilde}, the value iteration method is actually a MDP problem as above with the following parameters.

\begin{table}[!htbp]
\caption{Empirical MDP of table \ref{tab:queuing-1} and table \ref{tab:queuing-2}}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $0.5 \times 1 + 0.5 \times 5 = 3$ \\
    & Arrival rate $\lambda_{1}$ & $0.5 \times 0.13 + 0.5 \times 0.13 = 0.13$ \\
    & Departure rate $\mu_{1}$ & $0.5 \times 0.3 + 0.5 \times 0.3 = 0.3$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $2$ \\
    & Arrival rate $\lambda_{2}$ & $0.13$ \\
    & Departure rate $\mu_{2}$ & $0.3$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:mdp-queuing-1}
\end{table}

As in (??), the solution of table \ref{tab:mdp-queuing-1} is serving queue \(1\), which is also the result of the value iteration method on both table \ref{tab:queuing-1} and table \ref{tab:queuing-2}.

And finally, Q-learning also captures quite well these information and give us a similar result instead of choosing which queue has less cost in the initial environment (queue \(1\) for table \ref{tab:queuing-1} and queue \(2\) for table \ref{tab:queuing-2}) even though the speed of environment changing is slow.

We now move to a more complex case as follow:

\begin{table}[!htbp]
\caption{Queuing 3}
\begin{center}
\begin{tabular}{c c c c}
    \hline
    \multicolumn{2}{c}{} & $m_{1}$ & $m_{2}$ \\
    \cline{2-4}
    \multirow{3}{*}{$C_{1}$} & Holding cost & $3$ & $4$ \\
    & Arrival rate & $0.13$ & $0.12$ \\
    & Departure rate & $0.28$ & $0.31$ \\
    \cline{2-4}
    \multirow{3}{*}{$C_{2}$} & Holding cost & $2$ & $6$ \\
    & Arrival rate & $0.13$ & $0.14$ \\
    & Departure rate & $0.3$ & $0.32$ \\    
    \hline
\end{tabular}
\label{tab:queuing-3}
\end{center}
\end{table}

\begin{table}[!htbp]
\caption{Matrix transition of the environment of $Q_{1}$ of table \ref{tab:queuing-3}}
\begin{center}
\begin{tabular}{c c c}
    \hline
    & $m_{1}$ & $m_{2}$ \\
    $m_{1}$ & $*$ & $1$ \\
    $m_{2}$ & $2$ & $*$ \\
    \hline
    $\alpha$ & \multicolumn{2}{c}{$10^{-3}$} \\
    \hline
\end{tabular}
\end{center}
\label{tab:queuing-3-mat-1}
\end{table}

\begin{table}[!htbp]
\caption{Matrix transition of the environment of $Q_{2}$ of table \ref{tab:queuing-3}}
\begin{center}
\begin{tabular}{c c c}
    \hline
    & $m_{1}$ & $m_{2}$ \\
    $m_{1}$ & $*$ & $4$ \\
    $m_{2}$ & $3$ & $*$ \\
    \hline
    $\alpha$ & \multicolumn{2}{c}{$10^{-3}$} \\
    \hline
\end{tabular}
\end{center}
\label{tab:queuing-3-mat-2}
\end{table}

Using the same logic as above, we can transform it into a MDP problem as follow:

\begin{table}[!htbp]
\caption{Empirical MDP of table \ref{tab:queuing-3}}
\begin{center}
\begin{tabular}{c c c}
    \hline
    \multirow{3}{*}{$Q_{1}$} & Holding cost $c_{1}$ & $3.3333$ \\
    & Arrival rate $\lambda_{1}$ & $0.1266$ \\
    & Departure rate $\mu_{1}$ & $0.2899$ \\
    \cline{2-3}
    \multirow{3}{*}{$Q_{2}$} & Holding cost $c_{2}$ & $4.2857$ \\
    & Arrival rate $\lambda_{2}$ & $0.1357$ \\
    & Departure rate $\mu_{2}$ & $0.3114$ \\    
    \hline
\end{tabular}
\end{center}
\label{tab:mdp-queuing-3}
\end{table}



\begin{figure}

{\centering \subfloat[Online Q-learning\label{fig:20x20-queuing-3-1}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-queuing-3-1} }\subfloat[Offline Q-learning\label{fig:20x20-queuing-3-2}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-queuing-3-2} }\subfloat[Value iterations\label{fig:20x20-queuing-3-3}]{\includegraphics[width=0.33\linewidth]{report_files/figure-latex/20x20-queuing-3-3} }

}

\caption{Result of table \ref{tab:queuing-3} for 20x20 case. \(\gamma = 0.999\), \(\epsilon = 0.999\).}\label{fig:20x20-queuing-3}
\end{figure}

The result of value iterations on the right side is also the same as the value iterations of table \ref{tab:mdp-queuing-3}. Both actually gives the same numerical estimation of \(V_{*}\). Q-learning is also good at learning this type of system and we can see that it deduces a similar policy to the value iteration's one.

\newpage

\printbibliography

\end{document}
